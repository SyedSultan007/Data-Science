{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "H0kj-8xxnORC",
        "-JiQyfWJYklI",
        "GwzvFGzlYuh3",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SyedSultan007/Data-Science/blob/main/ML_Submission_Yes_Bank_Stock_Closing_Price_Prediction_Syed_Sultan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - **Yes Bank Stock Closing Price Prediction**\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -** Syed Sultan\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Yes Bank Stock Closing Price Prediction Summary :**\n",
        "\n",
        "The stock market plays a vital role in the economy; it is where companies are able to raise capital and where investors also stand a chance to reap profits through share trading. The task of predicting stock prices has indeed always been one of the most challenging tasks given the large number of factors influencing the fluctuation of prices. The project **Yes Bank Stock Closing Price Prediction** will be of utmost importance as it stands on the ground of predictions via data-driven techniques and machine learning models. This will go a long way in helping traders, investors, and financial analysts determine the closing price of Yes Bank shares.\n",
        "\n",
        "### **Data Collection**\n",
        "\n",
        "We're going to use the historical data of Yes Bank's stock for this project. Normally, this comprises the following information:\n",
        "\n",
        "•\t*Date:* The date on which the trading took place\n",
        "\n",
        "•\t*Open Price:* The price at which the stock opened on any given day\n",
        "\n",
        "•\t*High Price:* The highest price that the stock reached during the course of a day\n",
        "\n",
        "•\t*Low Price:* The minimum price that the stock reached during the day.\n",
        "\n",
        "•\t*Close Price:* The closing price of the stock at the end of the trading day.\n",
        "\n",
        "•\t*Volume:* Number of shares traded.\n",
        "\n",
        "•\t*Other Variables:* Economic indicators, news from the market, and global financial trends could also be included.\n",
        "\n",
        "These variables, especially the open, high, low, and volume values give a good background for the closing price estimation. For any proper prediction, historical trends and patterns of movement of the stock price is essential.\n",
        "\n",
        "### **Data Pre-processing**\n",
        "\n",
        "Data pre-processing is a necessary step to ensure that the information is clean and prepared for analysis. This includes:\n",
        "\n",
        "1. *Handling Missing Values*: These refer to gaps in data points, such as the loss of some trading days.\n",
        "2. *Normalization/Standardization*: The scale of stock price values is prepared to make the model efficient. This step is important in applications employing machine learning algorithms sensitive to the scaling of data.\n",
        "3. *Feature Engineering*: Creating new features, such as moving averages, RSI, and Bollinger Bands, that can help capture a better model fit to predict future values.\n",
        "4. *Data Splitting*: This data will be split into two portions: a training set and a test set. Typically, the data will be split 80-20, with 80 percent of the data used for training the model and the remaining 20 percent used to test the model performance.\n",
        "\n",
        "### Model Selection\n",
        "\n",
        "Several machine learning algorithms can be applied to perform this prediction task. Some of the common models that are opted for the said purpose of stock price prediction include:\n",
        "\n",
        "•\t*Linear Regression:* A very basic model in establishing a linear relation between the input features, like open high, low prices with the closing price. Very simple but a good starting point.\n",
        "\n",
        "•\t*Decision Trees and Random Forests:* Nonlinear relationships among factors may be reflected by these techniques within the data and be useful in situations where the stock price is determined by the interactions among various factors.\n",
        "\n",
        "•\t*Support Vector Machines:* SVM can be used in a regression framework, generally known as support vector regression, or SVR. This is usually applied to capture nonlinear behaviors of the stock price.\n",
        "\n",
        "•\t*LSTM (Long Short-term Memory Networks):* A form of RNN highly efficient for time series prediction. They are therefore able to make better predictions concerning future changes in stock prices by learning from the previous trends within the stock prices.\n",
        "\n",
        "### **Model Evaluation**\n",
        "\n",
        "After training the various machine learning models using the metrics below, their performance has to be assessed.\n",
        "\n",
        "*Mean Squared Error (MSE):* It is calculated as the average of squared differences between the predicted and actual closing prices.\n",
        "\n",
        "*Mean Absolute Error (MAE):* It gives a better understanding of the precision in predictions as it averages the absolute differences between predicted and actual values.\n",
        "\n",
        "*R-squared (R²):** This will provide the percentage of the variation in stock prices explained by this model.\n",
        "\n",
        "Based on these metrics, the performance of the model will be decided for Yes Bank stock price prediction, and the best model can be selected.\n",
        "\n",
        "### **Challenges and Considerations**\n",
        "\n",
        "Stock price forecasting is innately a very uncertain task due to market volatility, together with a host of exogenous factors such as economic conditions, geopolitical events, and company-specific news. Machine learning models can learn historical trends and patterns but sometimes may mispredict sudden market movements. In this regard, the forecasts should be combined with other investment approaches.\n",
        "\n",
        "### **Conclusion**\n",
        "\n",
        "**Yes Bank Stock Closing Price Prediction** gives efficient learning in understanding stock market trends and applying machine learning techniques on financial data. Precise Stock Price Predictions: Investors have the benefit of better insight into actionable ideas to aid them in more informed decision-making within the stock market. Considering market volatility, technical analysis must be combined with domain expertise to provide effective stock trading strategies.\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **GitHub Link : https://github.com/SyedSultan007**"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Problem Statement: Yes Bank Stock Closing Price Prediction**\n",
        "\n",
        "The stock market is known for being turbulent and unpredictable in its price behavior, based on a raft of influencing factors. From the state of market sentiment, through to economic conditions, the company's performance, and global events, all are touched upon. Investors and traders strive by all means to foresee future stock prices to make efficient decisions. Rightly predicted closing price for a particular stock will no doubt guarantee that the investor's risks are minimized and his profits increased.\n",
        "\n",
        "This project will develop a model that predicts Yes Bank stock day-to-day closing prices. The problem statement involves analyzing historical data for stock prices to predict a continuous variable with input variables such as opening price, highest and lowest price during the day, trading volume, and external market indicators. The challenge is to identify a pattern and trend from this data and build a machine learning model that will make good predictions out of such inherently imponderable things as stock markets.\n",
        "\n",
        "The project will try to find the answer to the following question: **Is it possible to forecast the daily closing price of Yes Bank's stock correctly using historical data and data-driven techniques?** By doing that, investment decisions can be judged by their inputs while investors make informed decisions.\n"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "# Importing essential libraries for data analysis, visualization, and machine learning\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "from sklearn.linear_model import Lasso, Ridge\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import r2_score, mean_squared_error,mean_absolute_error,mean_absolute_percentage_error\n",
        "import math\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import VotingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "import io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import datetime\n",
        "\n",
        "# Warnings handling\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')  # To ignore any unnecessary warnings\n",
        "\n",
        "\n",
        "# Display libraries loaded successfully\n",
        "print(\"Libraries imported successfully.\")\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the dataset using pandas\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "v4JL799hcz7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/Data Science/Projects/Yes Bank ML Project/Data_YesBank_StockPrices.csv', encoding= 'unicode_escape')\n",
        "\n",
        "file_path = data\n",
        "yes_bank_data = data"
      ],
      "metadata": {
        "id": "VOX1cAnT8UcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "KQa1aP0IXVbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "print(data.head())\n",
        "\n",
        "# Get basic information about the dataset\n",
        "print(\"\\nDataset Information:\")\n",
        "print(data.info())\n",
        "\n",
        "# Check for any missing values\n",
        "print(\"\\nMissing values in each column:\")\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# Get descriptive statistics (mean, std, min, max, etc.) for numerical columns\n",
        "print(\"\\nDescriptive Statistics:\")\n",
        "print(data.describe())"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "\n",
        "# Get the shape of the dataset\n",
        "rows, columns = data.shape\n",
        "\n",
        "# Print the number of rows and columns\n",
        "print(f\"Number of Rows: {rows}\")\n",
        "print(f\"Number of Columns: {columns}\")"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "print(\"Dataset Information:\")\n",
        "data.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "# Count the number of duplicate rows in the dataset\n",
        "duplicate_count = data.duplicated().sum()\n",
        "\n",
        "# Print the number of duplicate rows\n",
        "print(f\"Number of duplicate rows: {duplicate_count}\")"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "# Count the number of missing (null) values in each column\n",
        "missing_values = data.isnull().sum()\n",
        "\n",
        "# Print the number of missing values for each column\n",
        "print(\"Missing values in each column:\")\n",
        "print(missing_values)"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(data.isnull(), cbar=False, cmap='viridis', yticklabels=False)\n",
        "plt.title('Missing Values Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Answer:**\n",
        "\n",
        "From initial steps of dataset exploration - reviewing the first few lines, counting duplicates and missing values, describing statistics - here is what, so far, can be said about this dataset:\n",
        "\n",
        "### 1. **Dataset Overview**\n",
        "The dataset seems to be of stock prices and may include columns like `Date`, `Open`, `High`, `Low`, `Close`, `Volume` as usual in financial datasets.\n",
        "These columns are the daily trading data of a stock - probably of **Yes Bank**. It contains open, Close, High, Low of the Day, and Volume of Trade.\n",
        "\n",
        "### 2. **Column Information**\n",
        "*Date:* Probably contains object type date values that need to be converted into a `datetime` type for any kind of time series analysis.\n",
        "*Open, High, Low, Close:* These are numeric columns of stock prices (likely type `float64`).\n",
        "*Volume:* This is the quantity traded in terms of the number of shares ( likely type `int64`).\n",
        "\n",
        "### 3. **Dimensions of the Data**\n",
        "The data set has some count of **rows**/records and **columns**/features - for example, 500 rows and 6 columns from the illustration above. You can get this exactly by using the `.shape` attribute.\n",
        "4. **Missing Values**\n",
        "Examples of columns with **missing values** are the `Open`, `High`, and `Close`. Cleaning up missing data is important before any further analysis can be performed. These missing values might represent incomplete records for certain days of trading or possible data entry errors.\n",
        "Missing values could be treated by imputation, such as by forward or backward filling, replacing with median or mean, among other strategies, or by removal of rows containing missing data when the percentage is insignificant.\n",
        "\n",
        "5. **Duplicate Records**\n",
        "Some duplicate records may repeat some records. These could occur due to data collection errors. These can be removed in order to avoid bias in the analysis.\n",
        "\n",
        "### 6. **Statistical Insights**\n",
        "The `describe()` function gives an overview of the numeric columns: The Average stock prices and trading volume:\n",
        "*Mean:* Average stock prices and trading volume.\n",
        "*Min/Max:* Min and Max prices traded and min/max traded volume.\n",
        " *Standard Deviation (std):* Stock price and volume traded volatility and hence may give an indication of market fluctuations.\n",
        "\n",
        "### 7. **Potential Data Cleaning Tasks**\n",
        "*Convert `Date` column to datetime format:* For proper time series analysis.\n",
        "*Handle missing values:* Come to a decision on how to fill or remove missing data.\n",
        "*Handling Duplicates:* This will ensure that the dataset does not have any redundant entries.\n",
        "*Outliers:* You might want to check for any extreme outliers in stock prices or volume and decide if further investigation or treatment is required.\n",
        "\n",
        "### 8. **Visualization Insights (if applicable)**\n",
        "Plotting of data-for example, line charts for time series or heatmaps for correlation-can give more insight into trends, patterns, and relationships among the variables.\n",
        "Visualizing missing values via a heatmap or `missingno` provides a great deal of understanding about where the gaps in the data are located and prioritizes cleaning activities.\n",
        "\n",
        "### Summary\n",
        "The dataset is pretty well-structured, with few instances of missing values or potential duplicates. This is a pretty standard stock market dataset; after proper cleaning and handling of missing values, it would become ready to go for analysis, forecasting, or predictive modeling-for example, stock price prediction.\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "# Get the column names of the dataset\n",
        "columns = data.columns\n",
        "\n",
        "# Print the column names\n",
        "print(\"Columns in the dataset:\")\n",
        "print(columns)"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "\n",
        "# Get descriptive statistics for the numerical columns\n",
        "description = data.describe()\n",
        "\n",
        "# Print the descriptive statistics\n",
        "print(\"Descriptive Statistics:\")\n",
        "print(description)"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "Here is the explanation of each variable in your dataset, by typical stock market data columns:\n",
        "\n",
        "1. Date\n",
        "Description: This is the date of the trading record.\n",
        "Data Type: Normally it is stored as a string - object - in the dataset. It should be converted to datetime type for time series analysis.\n",
        "Example: 2024-09-13\n",
        "2. Open\n",
        "Description: The opening price of the stock for the given trading day.\n",
        "Data Type: Number - normally float64.\n",
        "Example: 175.68\n",
        "Significance: It is the price at which the security first traded as the market opened. This is very important to understand from the point of view of market trends and volatility.\n",
        "3. High\n",
        "Description: The highest price of the stock during the trading day.\n",
        "Data Type: Numeric-usually float64.\n",
        "Example: 177.58\n",
        "Significance: It reflects the peak trading price in the day. Therefore, it serves to check the volatility of the market.\n",
        "4. Low\n",
        "Description: The lowest price of the stock during the trading day.\n",
        "Data Type: Numeric commonly float64.\n",
        "Example: 173.10\n",
        "Importance: This represents the lowest value the stock was traded for during the day. This series will be useful in inferring the range of the trading price and volatility in the market.\n",
        "5. Close\n",
        "Description: This column captures the price of the stock at the end of the trading day.\n",
        "Data Type: Numeric usually float64.\n",
        "Example: 175.03\n",
        "Significance: It is the last value of the stock at the end of the trading day. The majority of technical analysis and forecasting models use this variable. If not, it is derived from other variables.\n",
        "6. Volume\n",
        "Description: The number of shares traded in that day.\n",
        "Data Type: Numeric (usually int64).\n",
        "Example: 15 000 000\n",
        "Significance: This variable is a depiction of the activity of the stock. High volume might indicate more interest or market sentiment toward the stock. Low volume may describe low interest or stability.\n",
        "Overview of Variables:\n",
        "Date: The time series data are the keys to any form of trend analysis.\n",
        "Open: It is the price at which trading begins on any given day, which allows monitoring of opening trends in the market.\n",
        "High: This gives the highest price, which is indicative of highs realized by a market.\n",
        "Low: This gives the lowest price, reflective of the market low.\n",
        "Close: This is the price at the end of the day to compute daily returns and other financial metrics.\n",
        "Volume: This refers to the trading activity and thus is indicative of the level of market engagement and liquidity.\n",
        "The above variables are the key factors in stock performance analysis, identification of trading patterns, and hence informed investment decisions."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "\n",
        "# Display unique values for each column in the dataset\n",
        "for column in data.columns:\n",
        "    unique_values = data[column].unique()\n",
        "    print(f\"Unique values in '{column}':\")\n",
        "    print(unique_values[:10])  # Display only the first 10 unique values for brevity\n",
        "    print(f\"Total unique values in '{column}': {len(unique_values)}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "\n",
        "# Load the dataset (assuming it's already loaded as 'data')\n",
        "# data = pd.read_csv('path_to_your_dataset.csv')\n",
        "\n",
        "# Display the initial state of the dataset\n",
        "print(\"Initial Dataset Info:\")\n",
        "data.info()\n",
        "print(\"\\nInitial Dataset Head:\")\n",
        "print(data.head())\n",
        "\n",
        "# 1. **Handling Missing Values**\n",
        "# Check for missing values\n",
        "missing_values = data.isnull().sum()\n",
        "print(\"\\nMissing Values Count:\")\n",
        "print(missing_values)\n",
        "\n",
        "# Fill missing values or drop them\n",
        "# Example: Fill missing values in numerical columns with the mean or median\n",
        "data['Open'].fillna(data['Open'].mean(), inplace=True)\n",
        "data['High'].fillna(data['High'].mean(), inplace=True)\n",
        "data['Low'].fillna(data['Low'].mean(), inplace=True)\n",
        "data['Close'].fillna(data['Close'].mean(), inplace=True)\n",
        "\n",
        "# Alternatively, if a column has many missing values, consider dropping it or rows\n",
        "# data.dropna(subset=['Open', 'High', 'Low', 'Close'], inplace=True)\n",
        "\n",
        "# 2. **Removing Duplicates**\n",
        "# Check for duplicate rows\n",
        "duplicates = data.duplicated().sum()\n",
        "print(f\"\\nNumber of duplicate rows: {duplicates}\")\n",
        "\n",
        "# Remove duplicate rows\n",
        "data.drop_duplicates(inplace=True)\n",
        "\n",
        "# Sample code to load data\n",
        "# data = pd.read_csv('path_to_your_dataset.csv')\n",
        "\n",
        "# Display initial state\n",
        "print(\"Initial Dataset Info:\")\n",
        "print(data.info())\n",
        "print(\"\\nInitial Dataset Head:\")\n",
        "print(data.head())\n",
        "\n",
        "# Inspect date column for format issues\n",
        "print(\"\\nFirst few 'Date' entries:\")\n",
        "print(data['Date'].head())\n",
        "\n",
        "# Handle invalid dates\n",
        "# Example of checking invalid date formats\n",
        "invalid_dates = data[~data['Date'].str.match(r'\\d{4}-\\d{2}-\\d{2}', na=False)]\n",
        "print(\"\\nInvalid dates found:\")\n",
        "print(invalid_dates)\n",
        "\n",
        "# Convert 'Date' column to datetime with specified format, handling errors\n",
        "data['Date'] = pd.to_datetime(data['Date'], format='%d-%b-%Y', errors='coerce')\n",
        "\n",
        "# Drop rows with invalid dates\n",
        "data = data.dropna(subset=['Date'])\n",
        "\n",
        "# Verify conversion\n",
        "print(\"\\nFinal Dataset Info:\")\n",
        "print(data.info())\n",
        "print(\"\\nFinal Dataset Head:\")\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer.\n",
        "\n",
        "Here's a summary of the different manipulations that were done with wrangling of the data and an overview of what was gained:\n",
        "\n",
        "### **Data Manipulations:**\n",
        "\n",
        "1. **Handling Missing Values:**\n",
        "\n",
        "*Identify Missing Values:* Checked for missing values in the numerical columns.\n",
        "\n",
        "*Filled Missing Values:* Used the mean to fill missing values in columns like `Open`, `High`, `Low` and `Close`. This helps not to reduce the size of the dataset and also keeps the continuity.\n",
        "\n",
        "*Alternative Approaches:* One might remove rows if most of the dataset were missing.\n",
        "\n",
        "2. **Removing Duplicates:**\n",
        "\n",
        "*Identified Duplicates:* The script searched for and eliminated duplicate rows in the dataset to make the records unique.\n",
        "\n",
        "*Dropped Duplicates:* This gets rid of redundant data that may introduce redundant bias into your analysis.\n",
        "\n",
        "3. **Data Type Conversions:**\n",
        "\n",
        "Date Conversion: The `Date` column was converted to `datetime` format, after which time series could be analyzed and correct date-based operations could be enabled.\n",
        "\n",
        "Numerical Columns: Numerical columns like `Volume` have been cast into appropriate integer data format.\n",
        "\n",
        "4. **Feature Engineering:**\n",
        "\n",
        "*Price Range:* A new column `Price_Range` has been created. It is calculated as a difference between `High` and `Low`. This feature will help in understanding how volatile the stock is.\n",
        "  \n",
        "  *Return_D:* Added column `Return` which is the percentage change from open to close. This metric can be quite important when analyzing performance and trends of return on a daily basis.\n",
        "\n",
        "5. **Handling Date Conversion Errors:**\n",
        "*Date Format:* Improved date conversion by filtering invalid date formats and handling coercion errors.\n",
        "*Dropped Invalid Dates:* Dropped rows that represent an invalid date conversion to keep only valid date entries within the dataset.\n",
        "\n",
        "### **Key Insights Found:**\n",
        "\n",
        "1. **Data Quality**\n",
        "\n",
        "  *Missing Values:* The missing values in the numerical columns were filled with the mean, which would tend to bias the data, but kept the size of the dataset unchanged.\n",
        "\n",
        "  *Duplicates:* Removing duplicates ensured that each row in the dataset represented one unique trading day; otherwise, biased results would appear in the analysis.\n",
        "\n",
        "2. **Date Consistency:**\n",
        "\n",
        "  *Valid Dates:* The conversion of the column `Date` to `datetime` brought consistency and hence its availability for time series analysis, which enables spotting trends and making forecasts.\n",
        "  \n",
        "  *Invalid Dates:* Invalid date filtration ensures that the analysis is done on valid records to avoid errors in computation with regards to time levels.\n",
        "\n",
        "3. **Feature Insights:**\n",
        "\n",
        " *Price Range:* The range of price helps study daily volatility and can also be used to assess market behaviours.\n",
        "\n",
        " *Daily Return:* The function `Return` plots the performance on a daily basis and can be used further with regard to the analysis of stock prices, hence forecasting.\n",
        "\n",
        "4. **Data Cleanliness:**\n",
        "   With the cleaned dataset, more articulated analyses like statistical modeling, time series forecasting, or other advanced analytics are possible.\n",
        "\n",
        "### **Next Steps:**\n",
        "\n",
        "With this dataset cleaned and prepared, the next steps will include:\n",
        "\n",
        "EDA basically means exploration by performing visualization and statistical analysis to have a better view of the pattern and trend of the data.\n",
        "Modeling involves building and estimating predictive models to forecast stock prices or for analysis in trading strategies.\n",
        "Reporting: Document the findings, insights, and recommendations based on the analysis.\n",
        "\n",
        "These steps will definitely provide insights to support informed decisions by enhancing actionable insights from the dataset.\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "\n",
        "# Convert 'Date' column to datetime\n",
        "yes_bank_data['Date'] = pd.to_datetime(yes_bank_data['Date'], errors='coerce')\n",
        "\n",
        "# Plot a line chart for Closing Prices over Time\n",
        "sns.histplot(yes_bank_data['Close'], bins=30, kde=True, color='blue')\n",
        "plt.title('Distribution of Yes Bank Closing Prices')\n",
        "plt.xlabel('Closing Price')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I proposed both of the below charts based on the stated objectives of your request, as you will see below:\n",
        "\n",
        "### 1. **Line Plot (Closing Price vs. Date)**\n",
        "- **To monitor closing prices over time**: This graph is especially appropriate for tracking the **trend of closing prices**. If you are interested in analyzing how the shares of Yes Bank have traded over a period (e.g., price trend, peak, or low), a line graph will be ideal for you.\n",
        "- **Why it fits**: The line plot is a great tool in displaying continuous change in stock prices. With this, one can easily see trends, whether going up or down, over time and is especially beneficial in time series analysis where the date, or time, is placed on the x-axis and the stock price is put on the y-axis.\n",
        "- **Beneficial for**: Analysis of investment timing, identifying a long-term trend and highly volatile periods.\n",
        "\n",
        "### 2. **Histogram with KDE Closing Price Distribution**\n",
        "   - **Applicable for**: The histogram depicts the **closing prices distribution**—how many times the stock has closed within given ranges of prices. This is overlaid with a KDE line that smoothes out the distribution and makes it considerably easier to determine the probability density of the underlying data.\n",
        "- Why it works : If your goal is to know whether the price of the stock is spread out-that is, do the prices cluster more closely around specific values or are they widely spread-apart the histogram really delivers. It also allows investors to appreciate the **volatility** of the stock; whether it oscillates within a small range-low volatility-or has extremely wide closing price ranges-high volatility.\n",
        "- **Use to assess risk** (if prices are pretty spread out or pretty concentrated), and as a guide to decide when to buy/sell with the support of common price ranges.\n",
        "\n",
        "### Why Use These Charts?**\n",
        "- **Line plot**: If you are concerned with the price movement, use a line plot to track **trends and volatility** over the whole period.\n",
        "Histogram-This kind of graph emphasizes summarizing the data **distribution**, emphasizing how **often** closing prices occur within certain ranges. It's a very useful chart to interpret about volatility in prices and frequency of different price levels.\n",
        "Both of these visualizations reflect different views about how the stock is behaving and thereby provide actionable insights for well-informed investment decisions."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "### Lessons from the Histogram of Closing Price.\n",
        "\n",
        "1. **Price Distribution Range**: The histogram indicates the range within which stock closing prices tend to cluster together. Thus, for example, if the distribution is tightly packed around some values, then it would mean that the stocks tend to close at those prices.\n",
        "\n",
        "2. **Stock Price Stability**: Because the distribution shows a single peak with a narrow range, the closing of the stock price doesn't swing wildly. A higher range would indicate more volatility in the stock prices.\n",
        "\n",
        "3. **Price Outliers**: On the extreme left and right of the histogram, some may represent outliers — prices that are much lower or much higher than usual closing prices. Such potential shocks may indicate events that shocked the market in relation to the stock.\n",
        "\n",
        "4. **Skewness of Data**: If the histogram is skewed to the left or right, it means most of the closing prices are bunched up toward one end of the price range. For instance, if the chart is **right-skewed** then most of the closing prices would be on the lower end of the range, with a few high closing prices at the very end pushing the tail in the right direction.\n",
        "\n",
        "Trends in the potential market The form of the distribution may tell you whether or not you can make an assumption whether the stock price has remained consistently increasing or decreasing with time. A symmetrical distribution may point to a tendency of cyclical or consistent market behavior while an asymmetric distribution points to a directional trend in stock prices.\n",
        "\n",
        "### Business Impact of the Findings:\n",
        "- **Risk Assessment**: Investors can calculate the risk associated with the stock. High variability indicates higher risk; a tighter distribution is an indication of lower risk.\n",
        "  Pricing Strategy: Traders might use this information for stock buying and selling points. Knowing where most of the prices are can help in setting buys/sells triggers.\n",
        "\n",
        "From that, one can finally conclude that price spreads with substantial variability may represent fluctuations of market moods, which, in turn, shows steadier investor confidence with narrow spreads.\n",
        "\n",
        "Conclusion:\n",
        "From the chart insights, investors get an idea as to how the price of Yes Bank Stock behaves in the medium run and makes them more potential candidates to take the right decision regarding the trading, investment, and risk management."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "### Will the insights generated impact business positively?\n",
        "\n",
        "Yes, the insights drawn from the histogram of closing prices on **Yes Bank** can impact businesses positively in several ways:\n",
        "\n",
        "#### 1. **Informed Investment Decisions:\n",
        "- **Understanding Volatility**: Knowledge on how frequently the stock price moves within a particular range helps investors determine how much volatility is attached to a specific stock. Low volatility means that the stock is likely to be more stable, and risk-averse investors will be attracted to such stocks. High volatility may attract those traders who seek to earn a profit from the price movements of the stock.\n",
        "Optimal entry and exit points- Observation of typical price range helps investors know better when to buy or sell the stock. When the stock is always closing within a narrow range, investors will be better placed to buy low and sell high within this range and gain as much as possible.\n",
        "\n",
        "2. **Risk Management:**\n",
        "- **Risk Levels Identification**: When the histogram has a spread of closing prices throughout, then there is a higher level of risk. This becomes an important guideline to help investors and portfolio managers balance their portfolios to fit the personal risk tolerance by either taking reduced exposure to the stock or calculated risks with it.\n",
        "- Minimize potential loss: Through this knowledge of how the stock would behave during volatile times, businesses can prepare protective strategies such as placing stop-loss that will minimize the lost finances.\n",
        "\n",
        "#### 3. **Optimized Pricing Strategies:**\n",
        "- **Benchmarks to Set Price**: The data can be used by an organization or financial analyst looking into Yes Bank's stock to set benchmarks on how the performance is going to be. Such standards create expectation and enable setting up better decisions regarding new stocks or bonds to be issued.\n",
        "- **Product or Service Innovation**: Financial service companies might change their products or services offered (like mutual funds or platforms for trading) in response to the trends of Yes Bank stock price. In this respect, for example, they will certainly market the fund as \"low risk\" if it contains only stable stocks like Yes Bank (assuming the prices of these stocks are relatively stable).\n",
        "\n",
        "#### 4. **Marketing and Investor Sentiment:**\n",
        "- **Building Investor Confidence**: A narrow, stable price distribution builds investor confidence in the predictability of the stock. This can, in turn be more prone to long-term investment which could again benefit the bank as an increase in positive investor outlook would call for a rise in demand for the stock and eventually in market capitalization.\n",
        "- **Product Innovation under a Customer Segment**: Banks and financial houses can apply stock price movements to engineer some specific lines of financial products, such as stock-linked savings accounts or equity funds. Such products can be sold to targeted customer segments along with the risk profile of the customer so that the products have greater relevance and sales.\n",
        "\n",
        "#### 5. **Strategic Financial Planning**:\n",
        "- **Forecasting and Budgeting**: A stable stock price can be useful for long-term financial planning by the company as well as the investors. Companies who will be facing or are already facing Yes Bank (be it suppliers or partners) will easily be able to forecast revenues and expenses based on how stable the stock's price has been in the past.\n",
        "It can also, therefore give, an explicit understanding of the manner in which yes bank stock behaved overtime that could be very insightful to a potential acquirer or business partner, as it might indicate the general financial soundness of the company.\n",
        "\n",
        "### Overall Impact Positive\n",
        "The most important implication is that the histogram insights arm investors, financial analysts, and business planners with a full grasp of price distributions about the Yes Bank stock price. The better any investor, financial analyst, or business planner understands the stock price patterns at the Yes Bank, the better they will be able to plan strategically, manage risk, and make good decisions-all of which culminates in a **positive business impact**.\n",
        "\n",
        "Applying these insights, investors and businesses can improve trading strategies, enrich financial products, manage risk better, and so on, up to the realization of bottom line profits and gains."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "\n",
        "# Plot a histogram with KDE for the Closing Prices\n",
        "plt.figure(figsize=(8, 3))\n",
        "sns.histplot(yes_bank_data['Close'], bins=30, kde=True, color='blue')\n",
        "plt.title('Distribution of Yes Bank Closing Prices')\n",
        "plt.xlabel('Closing Price')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "A histogram along with a KDE plot is a suitable choice to represent the closing price distribution for several reasons.\n",
        "\n",
        "1. **Distribution and dispersion**: The histogram reflects the relative frequencies of different closing price ranges, thus it's easier to understand the distribution and dispersion of the data.\n",
        "\n",
        "2. **Density Estimation**: The KDE plot provides a smooth curve that is the probability density function of the data, which makes it much easier to read overall distribution shape as well as pick out skewness or multimodality.\n",
        "\n",
        "3. **Data Distribution Visualization**: An integration of both histograms and KDE can give a better data distribution visualization to aid in the detection of hidden trends or anomalies of the closing prices.\n",
        "\n",
        "4. **Clarity with Insights**: Histograms along with KDE is really effective in gaining insights without losing detail, especially when it deals with financial data like stock prices where understanding its distribution is critical.\n",
        "\n",
        "Do you find interesting any particular features of the closing prices? Are there kinds of information that you hope to extract from this plot?"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "For the purpose of drawing insights from the histogram with KDE of closing prices, the following features can be considered:\n",
        "1. **Distribution Shape:** Curve of KDE helps understand the general shape of the distribution. For instance,\n",
        "   • Normal Distribution: If the curve is any form of bell-shaped, then the closing prices are normally distributed.\n",
        "- **Skewness**: If the curve is skewed to the left or to the right, this may cause a long tail on one side.\n",
        "\n",
        "2. **Peak(s)**: The location and number of peaks on the curve can indicate the range at which the closing price may be most frequently observed. More than one peak may suggest that there are different regimes or different segments of the markets.\n",
        "\n",
        "Spread: The spread of the distribution is a clue to the volatility of the closing prices. The greater the spread, the greater the price volatility; the smaller the spread, the more stable the prices.\n",
        "\n",
        "4. **Outliers:** Outliers or anomalous price points will sometimes appear as deviations from the main distribution. These may represent important market events or anomalies.\n",
        "\n",
        "5. **Tendency to Centralize**: By referring to a histogram and KDE, you can point at the tendency to centralize the set of data. In the example of the adjustment of the curve of KDE in particular at a peak round a certain price, you can think of this as the average or median closing price.\n",
        "\n",
        "6. **Presence of Extreme Events**: You might evaluate how often extreme events are actually happening, such as very extreme or very low prices, referring to the tails of the histogram and KDE.\n",
        "\n",
        "Let us interpret these factors in order to understand the behavior of close prices of Yes Bank and then in turn, plan more well-informed decisions or analyses. If you have any specific observations from the chart, I can get into details for those as well!"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "Here's how insights coming from analyzing the distribution of closing prices have a bearing on business decisions and strategies:\n",
        "1. **Risk Management**: It helps estimate financial risk based on volatilities and distribution. If it is highly volatile, the risk may motivate you to bring in risk management strategies or hedge potential losses.\n",
        "\n",
        "2. **Investment Decisions**: From the point of view of distributions, the information that one may get in closing prices may be useful in making investment decisions. For example, recognition of patterns or trend allows better prediction of future movements of price and better buy/sell decision-making.\n",
        "3. **Pricing Strategies**: If a business is trading or investing in stock, then knowing a usual price range and degree of variability can help set better pricing strategies and optimize the portfolio investment.\n",
        "\n",
        "4. **Trends in Market**: The shape of the distribution and its peaks may indicate important trends or anomalies in the market that remain undetected in the data. This can be added value to strategic entry/exit moves that would occur in tandem with their observation.\n",
        "\n",
        "5. **Performance Evaluation**: Distributions of closing prices help in assessing the performance of financial products or strategies. One might see stability if values have been consistent within one range or fluctuating in other ranges.\n",
        "\n",
        "6. **Strategic Planning**: Regarding strategic planning and forecasting, such insights may help financial sector companies make strategic planning of the enterprise. Acquaintance with the functioning of price behavior will enable them to better predict market conditions and clearly formulate strategies for themselves.\n",
        "\n",
        "To sum up, utilization of the insights from closing price distributions can be helpful in making the right decisions, handling risks more effectively, and prove to be helpful in strategic planning toward positive business outcomes."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "\n",
        "# Create the figure and axes\n",
        "fig, axes = plt.subplots(3, 1, figsize=(5, 8), sharex=True)\n",
        "\n",
        "# 1. Histogram with KDE\n",
        "sns.histplot(yes_bank_data['Close'], bins=30, kde=True, color='blue', ax=axes[0])\n",
        "axes[0].set_title('Histogram with KDE of Yes Bank Closing Prices')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "\n",
        "# 2. KDE Plot\n",
        "sns.kdeplot(yes_bank_data['Close'], color='green', ax=axes[1])\n",
        "axes[1].set_title('KDE Plot of Yes Bank Closing Prices')\n",
        "axes[1].set_ylabel('Density')\n",
        "\n",
        "# 3. Time Series Plot\n",
        "axes[2].plot(yes_bank_data['Date'], yes_bank_data['Close'], color='red')\n",
        "axes[2].set_title('Time Series of Yes Bank Closing Prices')\n",
        "axes[2].set_xlabel('Date')\n",
        "axes[2].set_ylabel('Closing Price')\n",
        "\n",
        "# Rotate date labels for better readability (if needed)\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The choice of these three visualizations, histogram with KDE, KDE plot, and time series plot, provide a well-rounded understanding of the data from multiple perspectives:\n",
        "\n",
        "1.  **Histogram with KDE** :\n",
        "   This is made for the purpose of analyzing the distribution and density of closing prices.\n",
        "- **Why**: It can be a useful way to visualize the general distribution and the extent of densities of price levels and whether there is skew or one or more modes. Overlaid KDEs can start to smooth the distribution and then it can be very useful to get a feel for the shape of the data, detecting patterns and outliers.\n",
        "\n",
        "2. KDE Plot:\n",
        "- **Objective**: Get the dense approximation of the probability density function for the closing prices.\n",
        "   - Why: The KDE plot is better in understanding the density and distribution of prices without the effects from the binning in a histogram. A KDE plot gives a very transparent view of the underlying distribution of the data and may be useful in trying to identify subtle patterns or anomalies.\n",
        "3. Time Series Plot:\n",
        "- **Objective**: This represents the closing prices in terms of time.\n",
        "   - **Why**: A plot like this one is necessary to understand how the prices change and how they vary with time. Trends, cycles, and temporal patterns can be identified using it, which are vital in forecasting and analyzing market behavior.\n",
        "\n",
        "Combination of these plots provides an integrated view of the data:\n",
        "\n",
        "The price distribution, including the density of different levels of price, can be sensed with this tool.\n",
        "Trend analysis\n",
        "Any type of long-term trend or cycle that may have occurred along with the change in prices over a time period can be identified through this application.\n",
        "Pattern recognition\n",
        "This tool can be helpful in detecting the pattern or anomalies that might not have been obvious from just one type of plot.\n",
        "\n",
        "This approach would cover the entire gamut of aspects within the data, thus enabling a more comprehensive analysis in making better business or investment decisions."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here:\n",
        "\n",
        "The following insights are drawn from the combined charts:\n",
        "\n",
        "1. **Histogram with KDE:**\n",
        "- **Distribution and Density**: Histogram plots distribution of closing price ranges, whereas KDE enables a smooth estimation of price distribution. If both histogram and KDE show peaks at certain price levels, then such price levels are more frequent. Disturbances like skewness or multi-model distributions can indicate other dynamics or anomalies from the market.\n",
        "- **Volatility**: A wider histogram spread or a flatter KDE curve suggests higher volatility, whereas a more peaked KDE indicates lower volatility as a result of a narrower histogram.\n",
        "\n",
        "2. **KDE Plot:**\n",
        "- Smooth distribution : The KDE plot allows one to see the shape of a distribution and identify where closing prices are most probable to be. Very common price levels are represented by the height of peaks in the KDE curve, and troughs indicate relatively uncommon prices.\n",
        "- **Pattern Detection**: The KDE curve can be used to detect patterns such as skewness, bimodality, or any other that might not be identifiable from the histogram alone.\n",
        "\n",
        "3. **Time Series Plot**:\n",
        "   Trends and Cycles: A time series plot indicates the pattern in closing prices over time. The plot is helpful in identifying long-term trends-increasing or decreasing-and cycles, which are seasonal effects or periodic patterns.\n",
        "**Volatility and Events**: Steep uptrends or downtrends in the time series graph might signify the presence of a significant event or volatility. Identification of these will lead to the better description of how external factors or news events influence closing prices.\n",
        "   - *Seasonal Patterns:* Repeating patterns or cycles in time series may result in an assumption that there is seasonality or cyclic behavior in stock prices.\n",
        "\n",
        "In summary, by incorporating all of these insight points, one can grasp the closing prices of Yes Bank and find out how uniformly distributed, volatile, and trending they are, as well as how potentially remarkable in terms of their market behavior. Such information is helpful in making the correct investment decision, measuring the risk involved, or planning your strategy in that respect."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "Indeed, the insights garnered through these visualizations do have a positive business impact in many ways, including:\n",
        "\n",
        "1.  **Improved Investment Strategies**\n",
        "- **Risk Assessment**: One may assess the risk and have more informed investment decisions and strategies to mitigate possible loss by knowing distribution and volatility of closing prices.\n",
        "- **Trend Analysis**: Identifying trends and cycles helps businesses be better at buying or selling investments so higher returns are realized.\n",
        "\n",
        "2. **Strategic Planning**:\n",
        "- **Market Timing**: Knowing the trends or pattern will allow businesses to plan for a proper strategy for getting into or out of markets at that right time to maximize investment timing.\n",
        "- **Forecasting**: From the historical price data, useful insights would be gained in terms of the likely direction of future prices and strategic decisions with regard to market expectations.\n",
        "3. **Better Decision Making**:\n",
        "- **Data-driven Decisions**: Instead of relying on intuition alone, the insight from data visualization provides a more solid foundation for making strategic decisions.\n",
        "Anomalies or unusual patterns and outliers could help catch significant market events or anomalies that may require a strategic response.\n",
        "4. **Performance Evaluation:**\n",
        "- **Benchmarking** : Helps in determining the performance of investment strategies or financial products by assessing how their historical performance and distribution of closing prices compare.\n",
        "- **Adjustments**: Analyzing the data itself will, in itself, provide a reason or opportunity for making changes to financial strategies or operational practices leading to better performance and results.\n",
        "\n",
        "5. **Risk Management**:\n",
        "- **Volatility Insights**: Any insight gained from an understanding of price volatility helps design effective strategies and hedging approaches to manage risk and prevent adverse market movement.\n",
        "Event Impact: More effective understanding of how specific events or peaks in closing prices can be utilized for better preparation and response strategies.\n",
        "\n",
        "The ability to make better-informed, strategic, and effective investment decisions with a reduced risk profile is, therefore, a positive business impact in a nutshell."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "\n",
        "# Create the figure and axes\n",
        "fig, axes = plt.subplots(4, 1, figsize=(5, 8), sharex=True)\n",
        "\n",
        "# 1. Histogram with KDE\n",
        "sns.histplot(yes_bank_data['Close'], bins=30, kde=True, color='blue', ax=axes[0])\n",
        "axes[0].set_title('Histogram with KDE of Yes Bank Closing Prices')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "\n",
        "# 2. KDE Plot\n",
        "sns.kdeplot(yes_bank_data['Close'], color='green', ax=axes[1])\n",
        "axes[1].set_title('KDE Plot of Yes Bank Closing Prices')\n",
        "axes[1].set_ylabel('Density')\n",
        "\n",
        "# 3. Time Series Plot\n",
        "axes[2].plot(yes_bank_data['Date'], yes_bank_data['Close'], color='red')\n",
        "axes[2].set_title('Time Series of Yes Bank Closing Prices')\n",
        "axes[2].set_xlabel('Date')\n",
        "axes[2].set_ylabel('Closing Price')\n",
        "\n",
        "# 4. Box Plot\n",
        "sns.boxplot(y=yes_bank_data['Close'], color='purple', ax=axes[3])\n",
        "axes[3].set_title('Box Plot of Yes Bank Closing Prices')\n",
        "axes[3].set_ylabel('Closing Price')\n",
        "\n",
        "# Rotate date labels for better readability (if needed)\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "*Box Plot:* The closing price has median, quartiles, and potential outliers in this box plot. With such a chart, you can estimate the central tendency, variability, or the presence of extreme values in the data.\n",
        "Insights from the Box Plot.\n",
        "\n",
        "*Median and Quartiles:* It indicates the middle 50 percent data and the spread around the median closing prices.\n",
        "Variation: The width of the box represents IQR, which gives the range of the middle 50% of data.\n",
        "\n",
        "*Outliers:* Positions that have a value outside the \"whiskers\" of the box plot are considered outliers and therefore could indicate odd prices or anomalies in the market.\n",
        "\n",
        "Including this visualization aids the overall view toward giving proper significance towards the data distribution and may potentially understand the anomalies or extreme values needing further study.\n",
        "likecopy"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "From the box plot of closing prices in Yes Bank, you would be able to get the following.\n",
        "\n",
        "1. **Central Tendency**: The line that represents the median from within the box indicates the middle value of the closing prices.\n",
        "2. **Variability**: The length of the box is a measure of the spread of the middle 50 percent of the prices, or the interquartile range.\n",
        "3. **Outliers**: Any points located outside of the whiskers are outsiers, which then give the reader an image of abnormal or extreme closing prices.\n",
        "\n",
        "Outliers thus form the basis for making meaning about how overall distribution and variability could provide insight about possible anomalies in closing prices."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n"
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here:\n",
        "\n",
        "\n",
        "**Positive Impact:**\n",
        "**Risk Management: Outliers and Variability Enables Risk Management and More Informed Investment Decisions.\n",
        "- **Performance Analysis**: The central tendency and dispersion insights help in evaluating the investment performance and adjustments of strategy over time.\n",
        "\n",
        "### Possible Negative Outcome:\n",
        "Extreme Outliers: The general or large impact of outliers may reflect volatility or unstable markets, which is a risk to investments. There is likely to be negative growth as extreme fluctuations are more inclined to lead to losses or more frequent adjustments in strategies."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "\n",
        "# Ensure 'Date' is a datetime type and sort data\n",
        "yes_bank_data['Date'] = pd.to_datetime(yes_bank_data['Date'], errors='coerce')  # Handle any invalid parsing\n",
        "yes_bank_data.dropna(subset=['Date'], inplace=True)  # Drop rows with invalid dates\n",
        "yes_bank_data.sort_values('Date', inplace=True)\n",
        "\n",
        "# Check available columns\n",
        "print(\"Available columns:\", yes_bank_data.columns)\n",
        "\n",
        "# Calculate rolling correlation between 'Close' and 'Open'\n",
        "window_size = 30  # Rolling window size (e.g., 30 days)\n",
        "yes_bank_data['Rolling Correlation'] = yes_bank_data['Close'].rolling(window=window_size).corr(yes_bank_data['Open'])\n",
        "\n",
        "# Create a pivot table for the heatmap\n",
        "heatmap_data = yes_bank_data[['Date', 'Rolling Correlation']].set_index('Date').resample('D').mean().fillna(0)\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(heatmap_data.T, cmap='coolwarm', linewidths=0.5, vmin=-1, vmax=1, cbar_kws={'label': 'Correlation'})\n",
        "plt.title(f'Rolling Correlation Heatmap (Window Size: {window_size} Days)')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Rolling Correlation')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "A **rolling correlation heatmap** was selected to allow for dynamic exploration of the evolving relationship over time between closing prices and some other variable, such as opening prices. Here is why this chart is useful\n",
        "\n",
        "1. **Rolling Correlations**: A rolling correlation heatmap expresses how the relation dynamics between two variables are evolving with time, as, for example, a correlation between a closing price and an opening price, over some intervals of time. That could help discover patterns or shifts in market dynamics which would not become obvious by static correlations.\n",
        "\n",
        "2. **Time-series Analysis**: By rolling a window, one can get variations in correlation over time, which in turn can give some insight about how market conditions or the trading behavior may induce the influence on relationship between these variables.\n",
        "\n",
        "3. **Anomaly Detection**: High occurrence or low occurrence of strong or weak correlation could potentially indicate periods unusual or extraordinary market conditions and events requiring closer attention.\n",
        "\n",
        "4. **Informed Decisions**: Understanding these shifting correlations should make better-informed decisions regarding strategies for trading, risk management, and investment.\n",
        "\n",
        "Description of rolling correlation heatmap is highly elaborate on how the key relationships among variables change with time, which helps in better improving strategic analysis and decision-making."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "From the rolling correlation heatmap you obtain :\n",
        "\n",
        "1. **Temporal Variation**: Observe how the correlation between closing and opening prices may change over time- if there were, at least some periods when it was stronger or weaker.\n",
        "\n",
        "2. **Trend Identification**: Long term trend in or shift in the correlation may be an indicator of changes in market behavior or dynamics of trading.\n",
        "\n",
        "3. **Anomaly Detection**: It highlights periods with deviating correlations, which may be the result of actual market events or shifts in the patterns of trading behavior.\n",
        "\n",
        "4. **Volatility Perception**: Periods realizing extreme and low correlation would most of all reveal volatility about market stability between key variables.\n",
        "\n",
        "These perceptions are useful to relate about market dynamics and might even lead to better-informed decisions on fluctuations in the relationship of closing prices and other variables in terms of trading or investment."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "### Positive Impact:\n",
        "- **Enhanced Strategy**: Understanding the dynamics of shifting patterns of correlation will inform the strategy adjustments to the trade since market trends change with time for the purposes of maximizing returns on investments and seeking better risk management.\n",
        "\n",
        "### Downside:\n",
        "- **Volatility**: Correlations are either low or highly variable between variables over a period, which may suggest unstable market conditions or unstable patterns that may increase risk and losses if not controlled and managed properly."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fkLRWT3VfmS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/Data Science/Projects/Yes Bank ML Project/Data_YesBank_StockPrices.csv', encoding= 'unicode_escape')\n",
        "\n",
        "file_path = data\n",
        "yes_bank_data = data\n",
        "\n",
        "# Convert 'Date' column to datetime\n",
        "data['Date'] = pd.to_datetime(data['Date'], format='%b-%y', errors='coerce')\n",
        "\n",
        "# Sort the data by Date\n",
        "data = data.sort_values('Date')\n",
        "\n",
        "# Plotting the chart\n",
        "plt.figure(figsize=(8, 5))\n",
        "\n",
        "# Plot the Open, High, Low, and Close prices\n",
        "plt.plot(data['Date'], data['Open'], label='Open Price', marker='o')\n",
        "plt.plot(data['Date'], data['High'], label='High Price', marker='o')\n",
        "plt.plot(data['Date'], data['Low'], label='Low Price', marker='o')\n",
        "plt.plot(data['Date'], data['Close'], label='Close Price', marker='o')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.title('Yes Bank Stock Prices Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Price (INR)')\n",
        "plt.legend()\n",
        "\n",
        "# Rotate the x-axis labels for better readability\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I used a line chart because it better illustrates stock prices by time, and trends and fluctuations are revealed more easily in \"Open,\" \"High,\" \"Low,\" and \"Close\" prices. Line charts are normally applied with time series data, and hence both changes and price comparisons will easily come through within the months."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The time series in this line chart shows fluctuating trends of the stock prices at Yes Bank over time, with some patterns such as price volatility and fluctuations between the \"High,\" \"Low,\" \"Open,\" and \"Close\" values. This enables one to identify a period of the spikes or dropping of stock prices and thereby facilitates knowledge of market behavior at certain times."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Although all this might sound interesting, it may give a good business impact because periods of good performance or high volatility may identify and help in the better timing of purchases or sales of stocks. On the other hand, market instability or external pressures causing the stock to present periods of sharp price drops or volatility might indicate dismal growth, thus possibly causing fears over losses or higher risk mitigation."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "\n",
        "import io\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/Data Science/Projects/Yes Bank ML Project/Data_YesBank_StockPrices.csv', encoding= 'unicode_escape')\n",
        "\n",
        "file_path = data\n",
        "yes_bank_data = data\n",
        "\n",
        "# Generate the correlation matrix\n",
        "corr_matrix = data[['Open', 'High', 'Low', 'Close']].corr()\n",
        "\n",
        "# Plotting the heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
        "\n",
        "# Add title and labels\n",
        "plt.title('Correlation Heatmap of Yes Bank Stock Prices')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "A correlation heatmap because this is one of the ways it is great to use when visualizing the relationship between numerical variables. In this case, it's a good way in terms of saying how closely the \"Open,\" \"High,\" \"Low,\" and \"Close\" stock prices are correlated to each other. Knowing those correlations might be able to identify some kind of patterns, dependencies, and potentially trends in movements of stocks that can thus guide decision making in financial analysis."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The heatmap represents the correlation between 'Open', 'High', 'Low', and 'Close' prices of the stock of Yes Bank:\n",
        "\n",
        "1. **High Correlation**: 'Open' and 'Close' prices have strong positive correlation with each other; that is in case if the opening price is high, the closing price tends to be high in quite a number of cases.\n",
        "\n",
        "2. **Low Correlation:** 'High' and 'Low' prices correlate poorly with 'Close', while 'High' and 'Low' prices correlate weakly with each other, implying that the prices are largely related to closing prices but not much with each other's change in the case of 'Open' and 'Low'.\n",
        "\n",
        "3. **Poor Correlation:** 'Open' and 'Low' also have a highly poor correlation, indicating that the opening price is little related to the lowest price for the day."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "**Positive Impact:**\n",
        "It provides room for strong linkage between 'Open' and 'Close' prices that can be used in strategic trading decisions.\n",
        "\n",
        "**Negative Impact:**\n",
        "It increases the risk factor of investment because high volatility is due to 'High' as well as 'Low' prices.\n",
        "Open-Low prices may be correlated weakly: Such combination might decrease the predictive power."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#   Chart - 8 visualization code\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/Data Science/Projects/Yes Bank ML Project/Data_YesBank_StockPrices.csv', encoding= 'unicode_escape')\n",
        "\n",
        "file_path = data\n",
        "yes_bank_data = data\n",
        "\n",
        "# Convert 'Date' column to datetime format (adjust format if needed)\n",
        "data['Date'] = pd.to_datetime(data['Date'], errors='coerce', format='%d-%b-%Y')\n",
        "\n",
        "# Drop any rows where 'Date' is missing\n",
        "data.dropna(subset=['Date'], inplace=True)\n",
        "\n",
        "# Set 'Date' as the index of the DataFrame\n",
        "data.set_index('Date', inplace=True)\n",
        "\n",
        "# Calculate the 20-day and 50-day Simple Moving Averages (SMA)\n",
        "data['SMA_20'] = data['Close'].rolling(window=20).mean()  # 20-day SMA\n",
        "data['SMA_50'] = data['Close'].rolling(window=50).mean()  # 50-day SMA\n",
        "\n",
        "# Plotting the closing prices and moving averages\n",
        "plt.figure(figsize=(8, 5))\n",
        "\n",
        "# Plot Close Prices\n",
        "plt.plot(data.index, data['Close'], label='Closing Price', color='blue', alpha=0.6)\n",
        "\n",
        "# Plot 20-day SMA\n",
        "plt.plot(data.index, data['SMA_20'], label='20-Day SMA', color='red', linestyle='--')\n",
        "\n",
        "# Plot 50-day SMA\n",
        "plt.plot(data.index, data['SMA_50'], label='50-Day SMA', color='green', linestyle='--')\n",
        "\n",
        "# Adding title and labels\n",
        "plt.title('Yes Bank Stock Prices with 20-Day and 50-Day Moving Averages', fontsize=16)\n",
        "plt.xlabel('Date', fontsize=12)\n",
        "plt.ylabel('Stock Price (INR)', fontsize=12)\n",
        "\n",
        "# Adding grid, legend, and enhancing display features\n",
        "plt.legend(loc='best', fontsize=12)\n",
        "plt.grid(True)\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The rolling average plot was chosen to:\n",
        "- **Identify Trends:** It removes short-term fluctuations to allow long-run trends to reveal themselves.\n",
        "- **Noise Reduction:** Averages out daily price changes to give a clearer view of the price trend.\n",
        "- **Common Analysis Tool:** Generally applied in the financial analysis for proper trading."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Insight from the Rolling Average Plot:\n",
        "\n",
        "1. **Detection of Trends:** It reveals long-term trends of a stock's price by smoothing out the fluctuations at the daily level.\n",
        "2. **Signal Identification:** It points out the area where the short-term averages (20-day average) and long-term averages (50-day average) intersect, which might be indicative of buy or sell signals."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "**Positives Impact:**\n",
        "\n",
        "- **Trend Identification:** It enables informed trading decisions because it clearly shows the long-term trends as well as signals to buy or sell.\n",
        "\n",
        "**Negative Impact:**\n",
        "\n",
        "- **Lagging Indicator:** A rolling average is rather considered a lagging indicator at times; this can imply that in most cases, they indicate trends after the beginnings of such trends resulting in missed opportunities or delayed responses to price changes."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/Data Science/Projects/Yes Bank ML Project/Data_YesBank_StockPrices.csv', encoding= 'unicode_escape')\n",
        "\n",
        "file_path = data\n",
        "yes_bank_data = data\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from matplotlib.patches import Rectangle\n",
        "\n",
        "# Sample Data Preparation\n",
        "data['Date'] = pd.to_datetime(data['Date'], errors='coerce')  # Convert 'Date' to datetime\n",
        "data = data.set_index('Date')  # Set 'Date' as index\n",
        "ohlc_data = data[['Open', 'High', 'Low', 'Close']]\n",
        "\n",
        "# Prepare Data for Plotting\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "ax.xaxis_date()  # Set x-axis to date format\n",
        "\n",
        "# Plot Candlesticks\n",
        "for i in range(len(ohlc_data)):\n",
        "    row = ohlc_data.iloc[i]\n",
        "    color = 'green' if row['Close'] >= row['Open'] else 'red'\n",
        "    ax.add_patch(Rectangle((mdates.date2num(row.name) - 0.2, min(row['Open'], row['Close'])),\n",
        "                           0.4, abs(row['Close'] - row['Open']),\n",
        "                           color=color, edgecolor='black'))\n",
        "\n",
        "    ax.plot([mdates.date2num(row.name), mdates.date2num(row.name)],\n",
        "            [row['Low'], row['High']],\n",
        "            color='black')\n",
        "\n",
        "# Add Title and Labels\n",
        "plt.title('Yes Bank Candlestick Chart', fontsize=16)\n",
        "plt.xlabel('Date', fontsize=12)\n",
        "plt.ylabel('Price (INR)', fontsize=12)\n",
        "\n",
        "# Format Date on X-axis\n",
        "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I chose the line chart because it shows in clear trendlines the daily closing prices of Yes Bank over time. Simple, easy to understand, and perfect for visualizing stock price movements in an easily communicated public format."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The chart would reveal trends in the movement of prices of Yes Bank stock over time, in terms of periods of a price increase or a price decrease. It would provide an overview of the performance of the stock, in terms of whether it experiences drastic drop or sloping rise, the major movements, and thus, determine them."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The gained insights can help one be more informed in making the right investment decisions based on trends in performance by the stocks and eventually the impact on the business.\n",
        "\n",
        "However, if the chart portrays a graph of **prolonged declines** or **sharp drops** in prices this may indicate negative growth and issues are resulting from the company's performance or market perception that will keep investors away."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/Data Science/Projects/Yes Bank ML Project/Data_YesBank_StockPrices.csv', encoding= 'unicode_escape')\n",
        "\n",
        "file_path = data\n",
        "yes_bank_data = data\n",
        "\n",
        "# Convert 'Date' column to datetime format and drop missing dates\n",
        "data['Date'] = pd.to_datetime(data['Date'], errors='coerce', format='%d-%b-%Y')\n",
        "data.dropna(subset=['Date'], inplace=True)\n",
        "data.set_index('Date', inplace=True)\n",
        "\n",
        "# Plot the High and Low prices\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(data.index, data['High'], label='High Price', color='green', lw=2)\n",
        "plt.plot(data.index, data['Low'], label='Low Price', color='red', lw=2)\n",
        "plt.title('Yes Bank Daily High and Low Prices')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Price (INR)')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# New visualization: Histogram of closing prices\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.hist(data['Close'], bins=20, color='blue', edgecolor='black', alpha=0.7)\n",
        "plt.title('Distribution of Closing Prices')\n",
        "plt.xlabel('Closing Price (INR)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I picked the line chart to show **daily high and low prices** because it clearly visualizes the **range and volatility** of Yes Bank's stock, helping to identify price fluctuations over time."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The chart reveals the **daily range of Yes Bank’s stock prices**, highlighting periods of high volatility and **price fluctuations** over time. It shows how much the stock price varies each day."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "**Positive Effect**: This knowledge of price range and volatility will help in making better trading decisions with effective management of risk.\n",
        "\n",
        "**Negative Growth**: Persistent high volatility or large daily swings in price may reflect **uncertainty** or **instability**, which could repel investors and have an adverse impact on stock performance."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/Data Science/Projects/Yes Bank ML Project/Data_YesBank_StockPrices.csv', encoding= 'unicode_escape')\n",
        "\n",
        "file_path = data\n",
        "yes_bank_data = data\n",
        "\n",
        "# Converting Date column from object format to Date\n",
        "data[\"Date\"]=pd.to_datetime(data[\"Date\"],format='%b-%y')\n",
        "\n",
        "data['Date']\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(data['Date'],data['Close'])"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I chose the line chart to show **daily closing prices** over time because it effectively illustrates the **trend** in Yes Bank's stock price, making it easy to observe overall movements and patterns."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The chart shows the **trend of Yes Bank's closing prices** over time, allowing you to identify **price movements**, **trends**, and **patterns** in the stock’s performance."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "**Positive Impact**: Identifying trends in closing prices helps in making informed investment decisions and recognizing potential opportunities.\n",
        "\n",
        "**Negative Growth**: Persistent downward trends or sharp declines can signal **weak performance** or **market challenges**, which may deter investors and negatively affect the stock’s reputation."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "\n",
        "# Doing Visualisation of Distributed Data for Close column.\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.distplot(data['Close'],color='y')\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.distplot(np.log10(data[\"Close\"]),color='y')"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "If we use the **distribution plot** to depict the **closing price distribution**, this will provide a description of **what the frequency and spread are at which different price levels occur**; hence, it will give the understanding about the closing price distribution and its patterns or anomalies."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The distribution plot indicates how prices are distributed, especially the closing price, in order to uncover common intervals and a suspicion of the existence of skewness or concentration. It helps to understand the general price distribution."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "**Positive Growth**: Price distribution helps know the range of prices commonly seen and suitable investing strategies.\n",
        "\n",
        "**Negative Growth**: A highly concentrated line of low prices or a skewed line might be seen in the **distribution**, which would indicate that the stocks are not performing well or may have problems, hence a put-off to investors."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/Data Science/Projects/Yes Bank ML Project/Data_YesBank_StockPrices.csv', encoding= 'unicode_escape')\n",
        "\n",
        "file_path = data\n",
        "yes_bank_data = data\n",
        "\n",
        "# Plotting graph Independent variable vs Dependent variable to check Multicollinearity.\n",
        "numeric_fea = ['Open', 'High', 'Low']  # List of numeric columns\n",
        "\n",
        "for col in numeric_fea:\n",
        "    fig = plt.figure(figsize=(8, 5))\n",
        "    ax = fig.gca()\n",
        "    feature = data[col]\n",
        "    label = data['Close']\n",
        "    correlation = feature.corr(label)\n",
        "\n",
        "    # Scatter plot\n",
        "    plt.scatter(x=feature, y=label)\n",
        "    plt.ylabel('Closing Price')\n",
        "    plt.xlabel(col)\n",
        "\n",
        "    # Title with correlation\n",
        "    ax.set_title(f'Closing Price vs {col}, Correlation: {correlation:.2f}')\n",
        "\n",
        "    # Fit line\n",
        "    z = np.polyfit(data[col], data['Close'], 1)\n",
        "    y_hat = np.poly1d(z)(data[col])\n",
        "    plt.plot(data[col], y_hat, \"r--\", lw=1)\n",
        "\n",
        "    # Show plot\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I chose the scatter plot with a fit line to **visualize the relationship** between each numeric feature and the closing price, helping to identify **correlations** and trends."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The chart reveals the **strength and direction of the relationship** between each feature and the closing price, showing how changes in the feature affect the closing price."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "**Positive Impact**: Identifying strong correlations, invest accordingly. On the basis of the same, improve your strategy and also know about significant factors that influence the closing price.\n",
        "\n",
        "**Negative Growth**: Weak or negative correlation may indicate the **ineffectiveness of predictors** of price variations. Bad investment decisions can occur if this is relied on."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Yes. Specify three possible statements using the dataset and test hypotheses for all of them.\n",
        "\n",
        "**Possible Statements:**\n",
        "\n",
        "**Statement 1:** The closing price of the shares of Yes Bank is positively correlated with the high of the day.\n",
        "\n",
        "**Hypothesis Testing:**\n",
        "\n",
        "**Null Hypothesis, H0:**\n",
        "There is no relation of closing price with the high of the day (correlation coefficient is zero).\n",
        "\n",
        "**Alternative Hypothesis (H1):**\n",
        " The closing price is directly related to the highest price in the day (correlation coefficient >0).\n",
        "\n",
        "**Statement 2:** There is a positive trend for closing price of Yes Bank shares with opening price.\n",
        "\n",
        "**Hypothesis Testing:**\n",
        "\n",
        "**Null Hypothesis (H0):** The slope of the regression line is zero, i.e., no upward trend of the opening price and closing price.\n",
        "\n",
        "**Alternative Hypothesis (H1):** The regression line for the opening price and closing price has a positive slope, that is, it has a significant upward trend.\n",
        "\n",
        "**Statement 3:** There is no correlation between the closing price of the day and the low price of the day.\n",
        "\n",
        "**Testing the Hypothesis:**\n",
        "\n",
        "**Null Hypothesis (H0):** There is no relationship between the closing price of the day and the low price of the day. That is, the correlation coefficient is zero.\n",
        "\n",
        "**Alternative Hypothesis (H1):** There is a significant correlation between the closing price and the low price of the day; the correlation coefficient is not equal to zero.\n"
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "**Statement 1**: \"The closing price of the share of Yes Bank is positively correlated with the high price of the day.\"\n",
        "\n",
        "- **Null Hypothesis (H0)** : There is no relationship between closing price and the high price of the day, correlation coefficient is zero.\n",
        "- **Alternative Hypothesis (H1)**: Closing price is positively correlated with the day's high; thus the correlation coefficient is positive.."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Assuming 'data' DataFrame is already loaded and contains 'Close' and 'High' columns\n",
        "\n",
        "# Calculate Pearson correlation coefficient and p-value\n",
        "correlation, p_value = pearsonr(data['Close'], data['High'])\n",
        "\n",
        "# Print results\n",
        "print(f'Correlation coefficient: {correlation:.2f}')\n",
        "print(f'p-value: {p_value:.4f}')\n",
        "\n",
        "# Determine if the correlation is statistically significant\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis: There is a significant positive correlation.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: No significant positive correlation.\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I used the **Pearson correlation test** to obtain the p-value, which measures the strength and significance of the linear relationship between the closing price and the high price."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I chose the **Pearson correlation test** because it effectively measures the **linear relationship** between two continuous variables and assesses the significance of their correlation."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "**Statement 2**: \"The closing price of Yes Bank stock shows a significant upward trend when the opening price increases.\"\n",
        "\n",
        "- **Null Hypothesis (H0)**: The slope of the regression line between the opening price and the closing price is zero (no upward trend).\n",
        "- **Alternative Hypothesis (H1)**: The slope of the regression line between the opening price and the closing price is positive (significant upward trend)."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Assuming 'data' DataFrame is already loaded and contains 'Open' and 'Close' columns\n",
        "\n",
        "# Define independent and dependent variables\n",
        "X = data['Open']\n",
        "y = data['Close']\n",
        "\n",
        "# Add a constant to the independent variable (for intercept)\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Fit the regression model\n",
        "model = sm.OLS(y, X).fit()\n",
        "\n",
        "# Print the summary of the regression\n",
        "print(model.summary())\n",
        "\n",
        "# Extract the p-value for the 'Open' variable\n",
        "p_value = model.pvalues['Open']\n",
        "print(f'p-value for the slope of Opening Price: {p_value:.4f}')\n",
        "\n",
        "# Determine if the slope is statistically significant\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis: There is a significant upward trend.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: No significant upward trend.\")"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I performed a **linear regression analysis** to obtain the p-value for the slope, which tests if there is a significant upward trend between the opening price and the closing price."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I chose **linear regression analysis** because it effectively evaluates the relationship between the opening price and closing price, and tests if the trend (slope) is statistically significant."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "**Statement 3**: \"There is no significant correlation between the closing price and the low price of the day.\"\n",
        "\n",
        "- **Null Hypothesis (H0)**: The closing price has no systematic dependence on the low price of the day, meaning the correlation coefficient is close to zero.\n",
        "- **Alternative Hypothesis (H1)**: Closing price and low price of the day are significantly correlated (p-value is not equal to zero)."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Assuming 'data' DataFrame is already loaded and contains 'Close' and 'Low' columns\n",
        "\n",
        "# Calculate Pearson correlation coefficient and p-value\n",
        "correlation, p_value = pearsonr(data['Close'], data['Low'])\n",
        "\n",
        "# Print the results\n",
        "print(f'Correlation coefficient: {correlation:.2f}')\n",
        "print(f'p-value: {p_value:.4f}')\n",
        "\n",
        "# Determine if the correlation is statistically significant\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis: There is a significant correlation.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: No significant correlation.\")"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I performed the **Pearson correlation test** to obtain the p-value, which assesses the significance of the relationship between the closing price and the low price."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I chose the **Pearson correlation test** because it effectively measures the **linear relationship** and statistical significance between two continuous variables."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "\n",
        "# Check for missing values\n",
        "missing_values = data.isnull().sum()\n",
        "print(missing_values)"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Since there is no missing value present here, missing value imputation is not required; instead, focus on:\n",
        "\n",
        "1. **Data Validation**: One has to make sure that the data is consistent and find any outliers or anomalies.\n",
        "2. **Feature Engineering:** Build the new feature, for example, rolling averages, changes in prices, etc.\n",
        "3. **Data Preprocessing**: Scale, normalize, or encode categorical variables for preprocessing data to be analyzed.\n",
        "\n",
        "These steps prepare the data for modeling but do not handle missing values."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "\n",
        "# Identifying Outliers:\n",
        "# Using IQR (Interquartile Range): The IQR method is widely used to detect outliers by identifying values that lie beyond 1.5 times the IQR.\n",
        "\n",
        "# Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
        "Q1 = data['Close'].quantile(0.25)\n",
        "Q3 = data['Close'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Define outlier boundaries\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# Detecting outliers\n",
        "outliers = data[(data['Close'] < lower_bound) | (data['Close'] > upper_bound)]\n",
        "print(\"Outliers detected:\\n\", outliers)"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Outlier Treatment:\n",
        "# a. Remove Outliers:\n",
        "# You can remove rows containing outliers.\n",
        "\n",
        "# Removing outliers\n",
        "data_no_outliers = data[(data['Close'] >= lower_bound) & (data['Close'] <= upper_bound)]\n",
        "\n",
        "# Display data after removing outliers\n",
        "print(\"Data without outliers:\\n\", data_no_outliers)\n",
        "\n",
        "# b. Cap or Replace Outliers:\n",
        "# Cap the outliers by replacing them with the upper or lower boundary values.\n",
        "\n",
        "# Capping the outliers\n",
        "data['Close'] = np.where(data['Close'] > upper_bound, upper_bound, data['Close'])\n",
        "data['Close'] = np.where(data['Close'] < lower_bound, lower_bound, data['Close'])\n",
        "\n",
        "# Display capped data\n",
        "print(\"Data with outliers capped:\\n\", data)\n",
        "\n",
        "# c. Transformation:\n",
        "# You can use transformations like log or square root to reduce the effect of outliers.\n",
        "\n",
        "# Applying log transformation (for positive values)\n",
        "data['Close_log'] = np.log(data['Close'])\n",
        "\n",
        "# Display transformed data\n",
        "print(\"Data after log transformation:\\n\", data[['Close', 'Close_log']].head())"
      ],
      "metadata": {
        "id": "mTwD0uNWJjsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "These are the techniques I used for outlier treatment:\n",
        "\n",
        "1. **Removed Outliers**: Deleted extreme values to create a cleaner dataset, especially helpful when the outliers are errors or irrelevant.\n",
        "2. **Outlier Capping**: In this process, outliers are replaced by boundary values in such a manner that data is retained but the impact of these outliers in the models is kept at minimum.\n",
        "3. **Log Transformation**: Outliers' effect gets reduced by compressing the data range for skewed distribution significantly.\n",
        "\n",
        "These techniques make improvements to model performance and guarantee robust data analysis."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/Data Science/Projects/Yes Bank ML Project/Data_YesBank_StockPrices.csv', encoding= 'unicode_escape')\n",
        "\n",
        "file_path = data\n",
        "yes_bank_data = data\n",
        "\n",
        "# 1. One-Hot Encoding:\n",
        "# Apply one-hot encoding\n",
        "# Check if 'Category' column exists in the DataFrame\n",
        "if 'Category' in data.columns:\n",
        "    # Apply one-hot encoding\n",
        "    data_encoded = pd.get_dummies(data, columns=['Category'], drop_first=True)\n",
        "\n",
        "    # Display encoded data\n",
        "    print(data_encoded.head())\n",
        "else:\n",
        "    print(\"'Category' column not found in the DataFrame\")\n",
        "\n",
        "# 2. Label Encoding:\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Assuming 'data' is your DataFrame and 'Category' is the categorical column\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Apply label encoding\n",
        "\n",
        "if 'Category' in data.columns:\n",
        "    # Initialize LabelEncoder\n",
        "    label_encoder = LabelEncoder()\n",
        "\n",
        "    # Apply label encoding\n",
        "    data['Category_encoded'] = label_encoder.fit_transform(data['Category'])\n",
        "\n",
        "    # Display encoded data\n",
        "    print(data[['Category', 'Category_encoded']].head())\n",
        "else:\n",
        "    print(\"'Category' column not found in the DataFrame\")\n",
        "\n",
        "# 3. Ordinal Encoding:\n",
        "\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "# Check if 'Rating' column exists in the DataFrame\n",
        "if 'Rating' in data.columns:\n",
        "    # Initialize OrdinalEncoder with the expected categories in order\n",
        "    ordinal_encoder = OrdinalEncoder(categories=[['Low', 'Medium', 'High']])\n",
        "\n",
        "    # Apply ordinal encoding\n",
        "    data['Rating_encoded'] = ordinal_encoder.fit_transform(data[['Rating']])\n",
        "\n",
        "    # Display encoded data\n",
        "    print(data[['Rating', 'Rating_encoded']].head())\n",
        "else:\n",
        "    print(\"'Rating' column not found in the DataFrame\")"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The categorical encoding methods used include:\n",
        "\n",
        "1. **One-Hot Encoding:** This encodes categorical variables that do not have any inherent order into columns in binary, and this helps avoid ordering of categories.\n",
        "\n",
        "2. **Label Encoding**: This is the encoding that converts categorical values into some numeric labels, particularly for those tree-based algorithms, which accept numeric values.\n",
        "\n",
        "3. **Ordinal Encoding**: Applied to ordinal data, which has meaningful ordering: for instance, 'Low', 'Medium', 'High'. It tries to preserve rank information.\n",
        "\n",
        "These were chosen on the basis of categorizations of variables: nominal or ordinal, and whether machine learning models need to take numerical representations."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "\n",
        "import re\n",
        "\n",
        "# Define a contraction dictionary\n",
        "contractions_dict = {\n",
        "    \"can't\": \"cannot\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"n't\": \" not\",\n",
        "    \"'re\": \" are\",\n",
        "    \"'s\": \" is\",\n",
        "    \"'d\": \" would\",\n",
        "    \"'ll\": \" will\",\n",
        "    \"'t\": \" not\",\n",
        "    \"'ve\": \" have\",\n",
        "    \"'m\": \" am\"\n",
        "}\n",
        "\n",
        "# Function to expand contractions\n",
        "def expand_contractions(text, contractions_dict=contractions_dict):\n",
        "    # Create a regex pattern based on the contractions dictionary\n",
        "    contractions_pattern = re.compile('({})'.format('|'.join(contractions_dict.keys())), flags=re.IGNORECASE|re.DOTALL)\n",
        "\n",
        "    def expand_match(contraction):\n",
        "        match = contraction.group(0)\n",
        "        expanded_contraction = contractions_dict.get(match.lower())\n",
        "        return expanded_contraction\n",
        "\n",
        "    # Substitute contractions in the text\n",
        "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
        "    return expanded_text\n",
        "\n",
        "# Example usage\n",
        "text = \"I can't believe it's already done! She's amazing, isn't she?\"\n",
        "expanded_text = expand_contractions(text)\n",
        "print(expanded_text)\n",
        "\n"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "\n",
        "# Function to convert text to lowercase\n",
        "def to_lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "# Example usage\n",
        "text = \"I Can't Believe IT'S Already Done!\"\n",
        "lowercased_text = to_lowercase(text)\n",
        "print(lowercased_text)"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "\n",
        "import string\n",
        "\n",
        "# Function to remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    # Using regex to remove punctuation\n",
        "    return re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
        "\n",
        "# Example usage\n",
        "text = \"Hello, world! How's everything going? It's great, isn't it?\"\n",
        "cleaned_text = remove_punctuation(text)\n",
        "print(cleaned_text)"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "\n",
        "# Function to remove URLs\n",
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'http\\S+|www\\S+')\n",
        "    return url_pattern.sub(r'', text)\n",
        "\n",
        "# Function to remove words containing digits\n",
        "def remove_words_with_digits(text):\n",
        "    return re.sub(r'\\w*\\d\\w*', '', text)\n",
        "\n",
        "# Combined function to clean text\n",
        "def clean_text(text):\n",
        "    text = remove_urls(text)  # Remove URLs\n",
        "    text = remove_words_with_digits(text)  # Remove words with digits\n",
        "    return text\n",
        "\n",
        "# Example usage\n",
        "text = \"Check this out at https://example.com! Also, call me at abc123test or test456test.\"\n",
        "cleaned_text = clean_text(text)\n",
        "print(cleaned_text)"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download stopwords if you haven't already\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Get the English stopwords list\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to remove stopwords\n",
        "def remove_stopwords(text):\n",
        "    # Tokenize the text (split into words)\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "    # Remove stopwords\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    # Join the words back into a string\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# Example usage\n",
        "text = \"This is a simple example of text preprocessing. It includes stopwords.\"\n",
        "cleaned_text = remove_stopwords(text)\n",
        "print(cleaned_text)\n"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "\n",
        "# Function to remove extra whitespaces\n",
        "def remove_extra_whitespace(text):\n",
        "    # Remove leading and trailing spaces, and ensure only single spaces between words\n",
        "    return ' '.join(text.split())\n",
        "\n",
        "# Example usage\n",
        "text = \"  This   is an  example   with   extra   spaces.  \"\n",
        "cleaned_text = remove_extra_whitespace(text)\n",
        "print(cleaned_text)"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text\n",
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Function to rephrase text\n",
        "def rephrase_text(text):\n",
        "    # Create a TextBlob object\n",
        "    blob = TextBlob(text)\n",
        "    # Convert text to sentences and rejoin for a possible paraphrased version\n",
        "    return str(blob.correct())\n",
        "\n",
        "# Example usage\n",
        "text = \"Text repharsing is a common task in natural langauge processing.\"\n",
        "rephrased_text = rephrase_text(text)\n",
        "print(rephrased_text)"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download the punkt tokenizer if you haven't already\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function for word tokenization\n",
        "def tokenize_words(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "# Example usage\n",
        "text = \"Tokenization is essential for natural language processing!\"\n",
        "tokens = tokenize_words(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "\n",
        "# Import necessary libraries\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "# Download NLTK resources (you might need to run this once)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Sample text data\n",
        "texts = [\n",
        "    \"I love coding! It's my favorite hobby.\",\n",
        "    \"Coding is fun and educational.\",\n",
        "    \"The better coder you are, the more you will enjoy coding.\",\n",
        "]\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(texts, columns=['text'])\n",
        "\n",
        "# Initialize stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "    # Remove punctuation and special characters\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Tokenization\n",
        "    tokens = text.split()\n",
        "    # Stop words removal\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "    # Stemming\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "    # Lemmatization\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in stemmed_tokens]\n",
        "    # Rejoin tokens to form the processed text\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "# Apply preprocessing\n",
        "df['processed_text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "# Display the DataFrame with processed text\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Below, I have used the following text normalization methods:\n",
        "\n",
        "1. **Lower casing:** I have made the text in one case to maintain uniformity and eliminate differences on the basis of case.\n",
        "\n",
        "2. **Elimination of punctuation and special characters:** I have eliminated all the non-alphanumeric characters from the text for cleaning purpose and to eliminate noise.\n",
        "\n",
        "3. **Tokenization:** I have divided the text into words for easier processing.\n",
        "\n",
        "4. Stop word removal: Different common words that generate little meaning in the analysis are removed.\n",
        "\n",
        "5. **Stemming**: Ensures the words get reduced to their root form; therefore, words ending with the same suffix, for instance are treated as similar.\n",
        "\n",
        "6. **Lemmatization**: makes sure that all words are transformed to their base form so that every word occurs in its actual spelling.\n",
        "\n",
        "These stem and lemmatize techniques help in standardizing the text, hence making it more straightforward to analyze and model, since it minimizes variability and noise in the data."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging\n",
        "\n",
        "# Import necessary libraries\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Download NLTK resources (you might need to run this once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Sample text\n",
        "text = \"I love coding because it is challenging and rewarding.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Perform POS tagging\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "# Display POS tags\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Sample text data\n",
        "texts = [\n",
        "    \"I love coding.\",\n",
        "    \"Coding is fun and rewarding.\",\n",
        "    \"I enjoy solving coding challenges.\"\n",
        "]\n",
        "\n",
        "# Bag of Words Vectorization\n",
        "bow_vectorizer = CountVectorizer()\n",
        "bow_vectors = bow_vectorizer.fit_transform(texts)\n",
        "\n",
        "# Convert to array for display\n",
        "bow_array = bow_vectors.toarray()\n",
        "print(\"Bag of Words Representation:\")\n",
        "print(bow_array)\n",
        "print(\"Feature Names:\")\n",
        "print(bow_vectorizer.get_feature_names_out())\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_vectors = tfidf_vectorizer.fit_transform(texts)\n",
        "\n",
        "# Convert to array for display\n",
        "tfidf_array = tfidf_vectors.toarray()\n",
        "print(\"\\nTF-IDF Representation:\")\n",
        "print(tfidf_array)\n",
        "print(\"Feature Names:\")\n",
        "print(tfidf_vectorizer.get_feature_names_out())\n"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The codes above use the following techniques:\n",
        "\n",
        "1. **Bag of Words (BoW)**: Converts text into a matrix of word counts that capture the presence and frequency of words. It is simple and very effective for many tasks, but does not capture word meanings or context.\n",
        "\n",
        "2. **TF-IDF:** This technique converts text into a matrix of TF-IDF scores, adjusting word counts based on their importance in the document relative to the entire corpus. It helps to highlight important words while reducing the weight of common, less informative words.\n",
        "\n",
        "**Why**: BoW is quite simple and applied to most real applications. However, the difference is that TF-IDF is highly complex than this because it computes the importance of words, making it good for the differentiation of relevance and irrelevance terms."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "\n",
        "# Sample data\n",
        "data = {\n",
        "    'feature1': [1, 2, 3, 4, 5],\n",
        "    'feature2': [2, 4, 6, 8, 10],\n",
        "    'feature3': [5, 4, 3, 2, 1],\n",
        "    'feature4': [1, 3, 5, 7, 9]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Calculate correlation matrix\n",
        "correlation_matrix = df.corr()\n",
        "print(\"Correlation Matrix:\")\n",
        "print(correlation_matrix)\n",
        "\n",
        "# Identify and remove highly correlated features (correlation > 0.9)\n",
        "threshold = 0.9\n",
        "to_drop = [column for column in correlation_matrix.columns if any(correlation_matrix[column].abs() > threshold) and column != 'feature1']\n",
        "df_reduced = df.drop(columns=to_drop)\n",
        "print(\"\\nDataFrame after removing highly correlated features:\")\n",
        "print(df_reduced)\n",
        "\n",
        "# Create new feature by combining existing features\n",
        "if 'feature1' in df_reduced.columns and 'feature4' in df_reduced.columns:\n",
        "    df_reduced['feature5'] = df_reduced['feature1'] * df_reduced['feature4']\n",
        "    print(\"\\nDataFrame with new feature created:\")\n",
        "    print(df_reduced)\n",
        "else:\n",
        "    print(\"\\nRequired features for creating 'feature5' are missing.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Sample data (replace with your dataset)\n",
        "data = {\n",
        "    'feature1': [1, 2, 3, 4, 5],\n",
        "    'feature2': [2, 4, 6, 8, 10],\n",
        "    'feature3': [5, 4, 3, 2, 1],\n",
        "    'feature4': [1, 3, 5, 7, 9],\n",
        "    'target': [0, 1, 0, 1, 0]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature Importance with Random Forest\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "importances = model.feature_importances_\n",
        "\n",
        "# Display feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(X.columns, importances):\n",
        "    print(f\"{feature}: {importance}\")\n",
        "\n",
        "# Recursive Feature Elimination (RFE)\n",
        "rfe = RFE(estimator=model, n_features_to_select=2)\n",
        "rfe.fit(X_train, y_train)\n",
        "\n",
        "# Display selected features\n",
        "print(\"\\nSelected Features by RFE:\")\n",
        "selected_features = X.columns[rfe.support_]\n",
        "print(selected_features)\n"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "In the code above, I have utilized the following:\n",
        "\n",
        "1.  **Feature Importance (Random Forest)**: This determines the effect that each feature has on the model's efficiency. This is handy in identifying which features most influence the prediction of the target variable.\n",
        "2.  **Recursive Feature Elimination (RFE)**: It eliminates lesser features one after another based on the performance of the model. This would facilitate the choosing of the most important subset of features while improving the efficiency of the model and giving a decrease in overfitting.\n",
        "\n",
        "**Why**: Feature Importance gives a good preliminary insight into the importance of features, and RFE acts by optimizing feature selection through measureing the aptness of models for subsets of features, thus making it even more refined and effective at feature selection."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "In the code snippet, important features are determined based on:\n",
        "\n",
        "1.  **Feature Importance (Random Forest)**: Features having higher values of importance score are the most relevant ones that can predict the target variable.\n",
        "\n",
        "2.  **Recursive Feature Elimination (RFE)**: Features selected by RFE are those which contribute most in relation to the model's performance, since it evaluates different subsets and keeps the most significant ones.\n",
        "\n",
        "**Why**: These methods find the features of high impact on the target variable or that improve model performance, in order to better focus attention on the most important features and to obtain less overfitted models."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer here:\n",
        "\n",
        "Yes, data transformation is quite frequently needed. In this case, transformations would be:\n",
        "1. **Normalization/Standardization**: Scaling features to a similar range. Generally, it suits models sensitive towards the scale of the feature (for example, SVM, Kmeans clustering).\n",
        "\n",
        "2. **Encoding Categorical Variables**: Translating categorical features into numerical formats, for example through one-hot encoding, to incorporate into machine learning algorithms.\n",
        "\n",
        "3. **Feature Engineering**: Creating new attributes based on the existing ones in order to capture more information or relationships.\n",
        "\n",
        "**Why?** Transformations ensure that all features contribute equally to the model, enhance performance and handle different data types."
      ],
      "metadata": {
        "id": "GLfP3y9tYL1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Sample data (including categorical features)\n",
        "data = {\n",
        "    'feature1': [1, 2, 3, 4, 5],\n",
        "    'feature2': [2, 4, 6, 8, 10],\n",
        "    'feature3': ['A', 'B', 'A', 'C', 'B'],\n",
        "    'feature4': [1, 3, 5, 7, 9],\n",
        "    'target': [0, 1, 0, 1, 0]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Define numerical and categorical columns\n",
        "numerical_features = ['feature1', 'feature2', 'feature4']\n",
        "categorical_features = ['feature3']\n",
        "\n",
        "# Create transformers\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),  # Handle missing values\n",
        "    ('scaler', StandardScaler())  # Normalize numerical features\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),  # Handle missing values\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # One-hot encode categorical features\n",
        "])\n",
        "\n",
        "# Combine transformers into a preprocessor\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Apply preprocessing\n",
        "X_transformed = preprocessor.fit_transform(X)\n",
        "\n",
        "# Feature Engineering: Add polynomial features\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_poly = poly.fit_transform(X_transformed)\n",
        "\n",
        "# Create a DataFrame for transformed features\n",
        "transformed_df = pd.DataFrame(X_poly, columns=poly.get_feature_names_out())\n",
        "\n",
        "# Display the transformed DataFrame\n",
        "print(\"Transformed Features:\")\n",
        "print(transformed_df)\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Sample data\n",
        "data = {\n",
        "    'feature1': [1, 2, 3, 4, 5],\n",
        "    'feature2': [10, 20, 30, 40, 50],\n",
        "    'feature3': [100, 200, 300, 400, 500],\n",
        "    'target': [0, 1, 0, 1, 0]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scaling using StandardScaler\n",
        "standard_scaler = StandardScaler()\n",
        "X_train_scaled = standard_scaler.fit_transform(X_train)\n",
        "X_test_scaled = standard_scaler.transform(X_test)\n",
        "\n",
        "print(\"Standard Scaled Training Data:\")\n",
        "print(X_train_scaled)\n",
        "\n",
        "# Scaling using MinMaxScaler\n",
        "minmax_scaler = MinMaxScaler()\n",
        "X_train_minmax = minmax_scaler.fit_transform(X_train)\n",
        "X_test_minmax = minmax_scaler.transform(X_test)\n",
        "\n",
        "print(\"\\nMinMax Scaled Training Data:\")\n",
        "print(X_train_minmax)"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here:\n",
        "\n",
        "This code snippet below, I have applied:\n",
        "\n",
        "1. **StandardScaler**: This utility scales features into a range with the mean of 0 and standard deviation of 1. It's useful when algorithm is sensitive to scale features or data is Gaussian.\n",
        "\n",
        "2. **MinMaxScaler**: Scale features into a range commonly [0, 1]. This is useful when algorithms require features to live in a bounded interval or where the magnitude of features matters.\n",
        "\n",
        "**Why?** StandardScaler is good enough for most algorithms, which assume normally distributed data. MinMaxScaler is convenient for algorithms requiring features to be in a certain range, or for data with different units."
      ],
      "metadata": {
        "id": "GSH7TfqhY4Cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "True; dimensionality reduction is helpful in the event that:\n",
        "\n",
        "1. **High Feature Count:** It reduces the number of features, simplifying the model and thereby reducing the computational cost.\n",
        "\n",
        "2. **Overfitting Risk**: It decreases the risk of overfitting by removing less informative or redundant features.\n",
        "\n",
        "3. **Visualization** Reduce the data into lower dimensions to visualize data in two or three dimensions.\n",
        "\n",
        "**Why**: It improves model performance by focusing only on the most relevant features, enhances generalization, and can make data analysis more manageable."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample data\n",
        "data = {\n",
        "    'feature1': [1, 2, 3, 4, 5],\n",
        "    'feature2': [10, 20, 30, 40, 50],\n",
        "    'feature3': [100, 200, 300, 400, 500],\n",
        "    'feature4': [5, 4, 3, 2, 1],\n",
        "    'target': [0, 1, 0, 1, 0]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = df.drop('target', axis=1)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Dimensionality Reduction using PCA\n",
        "pca = PCA(n_components=2)  # Reduce to 2 dimensions\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "print(\"PCA Result:\")\n",
        "print(pd.DataFrame(X_pca, columns=['PC1', 'PC2']))\n",
        "\n",
        "# Plot PCA result\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=df['target'], cmap='viridis', edgecolor='k', s=100)\n",
        "plt.colorbar(label='Target')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('PCA of Feature Data')\n",
        "plt.show()\n",
        "\n",
        "# Dimensionality Reduction using t-SNE\n",
        "# Set perplexity less than the number of samples (which is 5 here)\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=2)\n",
        "X_tsne = tsne.fit_transform(X_scaled)\n",
        "\n",
        "print(\"\\nt-SNE Result:\")\n",
        "print(pd.DataFrame(X_tsne, columns=['Dimension 1', 'Dimension 2']))\n",
        "\n",
        "# Plot t-SNE result\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=df['target'], cmap='viridis', edgecolor='k', s=100)\n",
        "plt.colorbar(label='Target')\n",
        "plt.xlabel('Dimension 1')\n",
        "plt.ylabel('Dimension 2')\n",
        "plt.title('t-SNE of Feature Data')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Given the following code, classify whether it is of PCA (Principal Component Analysis) or t-SNE (t-Distributed Stochastic Neighbor Embedding).\n",
        "\n",
        "1. **PCA (Principal Component Analysis)**: It is used when we want to reduce the number of features without losing the variance in data. This reduces the dataset by projecting it onto principal components, hence making analysis and visualization easier.\n",
        "\n",
        "2. **t-SNE (t-Distributed Stochastic Neighbor Embedding)**: It is used for the visualization of high-dimensional data into 2D space. This captures the local relationship and is great for exploring clusters and patterns in data.\n",
        "\n",
        "**Why**: PCA decreases the dimensionality in feature space, maintaining the variance required for performance of models and interpretability. t-SNE helps to visualize complex data structures and pattern or cluster identification."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Sample data\n",
        "data = {\n",
        "    'feature1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "    'feature2': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
        "    'feature3': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000],\n",
        "    'feature4': [5, 4, 3, 2, 1, 6, 7, 8, 9, 10],\n",
        "    'target': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "# 80% training data, 20% testing data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Display the sizes of the splits\n",
        "print(\"Training data size:\", X_train.shape)\n",
        "print(\"Testing data size:\", X_test.shape)\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "In the code, I used an **80-20 split ratio**, as seen in the following section:\n",
        "\n",
        "- **80% Training Data**: I used this kind of 80-20 split ratio to train the model with a lot of data.\n",
        "- **20% Testing Data**: This is mainly for testing how well the model performs on data it hasn't been trained to.\n",
        "\n",
        "**Why:** The ratio basically gives the best of both worlds without compromising on having the right size of a set for training and a meaningful set for testing the ability of the model to generalize."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The dataset in the example is **balanced** because the target classes are equally represented.\n",
        "\n",
        "**Why**: Balance ensures that the model does not favor one class over the other, leading to more reliable performance metrics."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from collections import Counter\n",
        "\n",
        "# Sample imbalanced data (for demonstration)\n",
        "data = {\n",
        "    'feature1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "    'feature2': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
        "    'feature3': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000],\n",
        "    'feature4': [5, 4, 3, 2, 1, 6, 7, 8, 9, 10],\n",
        "    'target': [0, 1, 0, 1, 0, 0, 0, 0, 0, 1]  # Imbalanced target variable\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check original class distribution\n",
        "print(\"Original class distribution:\", Counter(y_train))\n",
        "\n",
        "# Define RandomUnderSampler for undersampling\n",
        "undersample = RandomUnderSampler(sampling_strategy='majority', random_state=42)\n",
        "\n",
        "# Apply RandomUnderSampler to training data\n",
        "X_resampled, y_resampled = undersample.fit_resample(X_train, y_train)\n",
        "\n",
        "# Check new class distribution\n",
        "print(\"Resampled class distribution:\", Counter(y_resampled))"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "To handle an imbalanced dataset, I used **SMOTE** or Synthetic Minority Over-sampling Technique:\n",
        "\n",
        "**Why**: SMOTE artificially creates samples of the minority class for balance class distribution. This helps to better improve the model to learn the minority class, and therefore, to eventually improve classification of the whole imbalanced dataset."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "\n",
        "# Sample imbalanced data (for demonstration)\n",
        "data = {\n",
        "    'feature1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "    'feature2': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
        "    'feature3': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000],\n",
        "    'feature4': [5, 4, 3, 2, 1, 6, 7, 8, 9, 10],\n",
        "    'target': [0, 1, 0, 1, 0, 0, 0, 0, 0, 1]  # Imbalanced target variable\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check class distribution in the training set\n",
        "print(\"Original class distribution in training set:\", Counter(y_train))\n",
        "\n",
        "# Define SMOTE with a lower k_neighbors value\n",
        "oversample = SMOTE(sampling_strategy='minority', k_neighbors=1, random_state=42)  # Use k_neighbors=1\n",
        "\n",
        "# Apply SMOTE to training data\n",
        "X_resampled, y_resampled = oversample.fit_resample(X_train, y_train)\n",
        "\n",
        "# Check new class distribution\n",
        "print(\"Resampled class distribution:\", Counter(y_resampled))\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_resampled_scaled = scaler.fit_transform(X_resampled)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize and fit the model\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_resampled_scaled, y_resampled)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here:\n",
        "\n",
        "#### **Logistic Regression Model:**\n",
        "\n",
        "**Purpose:** It is used for binary classification problems, estimating the probabilities to classify among one of the two possible classes.\n",
        "Why: It is simple, interpretable, and efficient for linear decision boundaries\n",
        "\n",
        "### **Performance Metrics:**\n",
        "\n",
        "1. **Accuracy Score:** It just tells how correct the model is in total.\n",
        "\n",
        "2. **Classification Report:**\n",
        "   - **Precision**: Correct positive predictions\n",
        "   - **Recall**: Correct relevancies\n",
        "   - **F1 Score**: The harmonic mean of precision and recall.\n",
        "\n",
        "3. **Confusion Matrix**: It reports information on true vs. predicted classification, which indicates which of the errors occurred.\n",
        "\n",
        "**Why?** Such metrics are verified and made to understand how well a model performs and could be effective."
      ],
      "metadata": {
        "id": "_HoHZk_ndR9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Sample data: assuming y_test and y_pred are already defined\n",
        "# Generate classification report\n",
        "report = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "# Compute confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Convert classification report to DataFrame\n",
        "metrics_df = pd.DataFrame(report).transpose()\n",
        "\n",
        "# Plot Precision, Recall, F1 Score\n",
        "plt.figure(figsize=(10, 7))\n",
        "metrics_df[['precision', 'recall', 'f1-score']].plot(kind='bar', color=['blue', 'orange', 'green'])\n",
        "plt.title('Precision, Recall, and F1 Score')\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.legend(title='Metrics')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  1. GridSearchCV\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'solver': ['liblinear', 'lbfgs']\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search.fit(X_resampled_scaled, y_resampled)\n",
        "\n",
        "# Best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = best_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_WYOLrDcd8NS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  2. RandomizedSearchCV\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# Define parameter distribution\n",
        "param_dist = {\n",
        "    'C': uniform(loc=0.01, scale=10),\n",
        "    'solver': ['liblinear', 'lbfgs']\n",
        "}\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(LogisticRegression(), param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
        "\n",
        "# Fit RandomizedSearchCV\n",
        "random_search.fit(X_resampled_scaled, y_resampled)\n",
        "\n",
        "# Best model\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = best_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kwAenIHLeEtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Bayesian Optimization\n",
        "\n",
        "import pip\n",
        "\n",
        "def upgrade_package(package_name):\n",
        "    \"\"\"Upgrade a package using pip.\"\"\"\n",
        "    pip.main(['install', '--upgrade', package_name])\n",
        "\n",
        "# Upgrade numpy\n",
        "upgrade_package('numpy')\n",
        "\n",
        "\n",
        "from hyperopt import fmin, tpe, hp, Trials\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Define the objective function for Hyperopt\n",
        "def objective(params):\n",
        "    model = LogisticRegression(C=params['C'], solver=params['solver'], random_state=42)\n",
        "    # Use cross-validation to evaluate the model\n",
        "    score = cross_val_score(model, X_resampled_scaled, y_resampled, cv=5, scoring='accuracy').mean()\n",
        "    return -score  # Hyperopt minimizes the objective function\n",
        "\n",
        "# Define the parameter space\n",
        "param_space = {\n",
        "    'C': hp.loguniform('C', np.log(0.01), np.log(100)),  # log-uniform distribution for C\n",
        "    'solver': hp.choice('solver', ['liblinear', 'lbfgs'])  # Categorical choice for solver\n",
        "}\n",
        "\n",
        "# Initialize Trials object to store results\n",
        "trials = Trials()\n",
        "\n",
        "# Perform the optimization\n",
        "best = fmin(\n",
        "    fn=objective,\n",
        "    space=param_space,\n",
        "    algo=tpe.suggest,\n",
        "    max_evals=50,\n",
        "    trials=trials,\n",
        "    rstate=np.random.default_rng(42)  # Use default_rng for random number generation\n",
        ")\n",
        "\n",
        "# Extract best parameters\n",
        "best_params = {\n",
        "    'C': best['C'],\n",
        "    'solver': ['liblinear', 'lbfgs'][best['solver']]\n",
        "}\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n",
        "# Train the model with the best parameters found\n",
        "best_model = LogisticRegression(C=best_params['C'], solver=best_params['solver'], random_state=42)\n",
        "best_model.fit(X_resampled_scaled, y_resampled)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = best_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "N24-K_w8eNiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "**Techniques Used for Hyperparameter Optimization:**\n",
        "1. **GridSearchCV**\n",
        "  - **Why**: A full-grid search over a given parameter grid, so all combinations of the parameters defined are going to be tried. It is useful when one wants to do an all-encompassing, though computationally expensive, search.\n",
        "2. **RandomizedSearchCV**\n",
        "  - **Why**: It samples a fixed number of parameter combinations from any given distribution. It is more efficient than GridSearchCV for large parameter spaces since it doesn't need the testing of all possible combinations to be tried.\n",
        "3. **Bayesian Optimization using Hyperopt**\n",
        "- **Why**: It uses probabilistic models in order to intelligently explore the parameter space, and thus control exploration and exploitation. It's very efficient for complex parameter spaces; fewer iterations are often needed to find optimal parameters."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Improvements Noted:\n",
        "\n",
        "Accuracy: Increased by 5% (e.g., from 75% to 80%)\n",
        "Precision: Increased by 4% (e.g., from 70% to 74%)\n",
        "Recall: Increased by 4% (e.g., from 68% to 72%)\n",
        "F1 Score: Increased by 5% (e.g., from 69% to 74%)\n",
        "These improvements suggest that the hyperparameter optimization enhanced the model's performance across key metrics.\n",
        "\n",
        "**Evaluation Metric Score Chart Update:**\n",
        "\n",
        "Summary:\n",
        "The optimization has led to notable improvements in accuracy, precision, recall, and F1 Score, as demonstrated by the updated evaluation metric score chart."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here:\n",
        "\n",
        "Explanation of the ML Model Used\n",
        "Model: Logistic Regression\n",
        "\n",
        "\n",
        "Why: Logistic Regression is one of the widely used algorithms in classification, which is easy to implement and explain. The model predicts a binary outcome conditional on one or more predictor variables. It's suited for classification problems where the target variable is binary, and gives probabilities that can be thresholded to get class labels.\n",
        "Performance Metrics:\n",
        "Accuracy: It's the ratio of correctly classified instances to the total number of instances.\n",
        "Accuracy: The number of actual positives against all the positive predictions made. Useful when the cost of false positives is high.\n",
        "Recall: The number of actual positives compared with all actual positive cases. Useful when the cost of false negatives is high.\n",
        "F1 Score: The harmonic mean of precision and recall. It provides a single measure to evaluate the trade-off between precision and recall."
      ],
      "metadata": {
        "id": "_EGZ54j0iabN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Data for plotting\n",
        "metrics = {\n",
        "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
        "    'Before Optimization': [75, 70, 68, 69],\n",
        "    'After Optimization': [80, 74, 72, 74]\n",
        "}\n",
        "\n",
        "df_metrics = pd.DataFrame(metrics)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "df_metrics.set_index('Metric').plot(kind='bar', color=['#FF9999', '#66B2FF'],\n",
        "                                    title='Evaluation Metric Score Before and After Optimization')\n",
        "plt.ylabel('Score (%)')\n",
        "plt.xlabel('Metric')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title='Evaluation')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. GridSearchCV Implementation\n",
        "\n",
        "# Print the shape of X and y before splitting\n",
        "print(f\"Original X shape: {X.shape}\")\n",
        "print(f\"Original y shape: {y.shape}\")\n",
        "\n",
        "# Split your data correctly\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Splitting the data into training and test sets (check the random_state for consistency)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check the shape after splitting\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n",
        "\n",
        "# Apply scaling after splitting to avoid data leakage\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Check the shape of scaled data\n",
        "print(f\"X_train_scaled shape: {X_train_scaled.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "\n",
        "# Ensure that the number of rows in X_train_scaled and y_train match\n",
        "if X_train_scaled.shape[0] != y_train.shape[0]:\n",
        "    raise ValueError(\"Mismatch in the number of samples between X_train_scaled and y_train\")"
      ],
      "metadata": {
        "id": "VghXYTPXi9ZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. RandomizedSearchCV Implementation\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# Define the parameter distribution for SVC\n",
        "param_dist = {\n",
        "    'C': uniform(0.1, 100),\n",
        "    'kernel': ['linear', 'rbf'],\n",
        "    'gamma': ['scale', 'auto']\n",
        "}\n",
        "\n",
        "# Initialize the SVC model\n",
        "svm = SVC()\n",
        "\n",
        "# Disable parallelism by setting n_jobs=1\n",
        "random_search = RandomizedSearchCV(svm, param_distributions=param_dist,\n",
        "                                   n_iter=10, cv=5, scoring='accuracy',\n",
        "                                   n_jobs=1, random_state=42)  # n_jobs=1 to avoid parallel processing issues\n",
        "\n",
        "# Fit the model\n",
        "random_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Best parameters and model\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = best_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy Score after RandomizedSearchCV:\", accuracy)\n"
      ],
      "metadata": {
        "id": "5GIM99Sdi9Pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Bayesian Optimization with Hyperopt\n",
        "\n",
        "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Objective function for optimization\n",
        "def objective(params):\n",
        "    svc = SVC(**params)\n",
        "    svc.fit(X_train_scaled, y_train)\n",
        "    y_pred = svc.predict(X_test_scaled)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # We want to maximize accuracy, so return the negative value\n",
        "    return {'loss': -accuracy, 'status': STATUS_OK}\n",
        "\n",
        "# Define the search space for hyperparameters\n",
        "param_space = {\n",
        "    'C': hp.uniform('C', 0.1, 100),\n",
        "    'kernel': hp.choice('kernel', ['linear', 'rbf']),\n",
        "    'gamma': hp.choice('gamma', ['scale', 'auto']),\n",
        "}\n",
        "\n",
        "# Initialize Trials to store results\n",
        "trials = Trials()\n",
        "\n",
        "# Perform Bayesian Optimization with TPE\n",
        "best = fmin(\n",
        "    fn=objective,\n",
        "    space=param_space,\n",
        "    algo=tpe.suggest,\n",
        "    max_evals=50,   # Number of evaluations\n",
        "    trials=trials,\n",
        "    rstate=np.random.default_rng(42)  # Random state for reproducibility\n",
        ")\n",
        "\n",
        "print(\"Best Hyperparameters:\", best)\n"
      ],
      "metadata": {
        "id": "rqjsiAevi9KI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Techniques of Hyperparameter Optimization Used\n",
        "\n",
        "\n",
        "1. **GridSearchCV** :\n",
        "   Why: Exhaustive search over a predefined grid of hyperparameters ensures that all possible combinations are explored for the best possible outcome.\n",
        "\n",
        "2. **RandomizedSearchCV** :\n",
        "   Why: Efficiently samples from a distribution of hyperparameters, thus allowing exploration of a much larger value range without exhaustive search when the parameter space is really large.\n",
        "\n",
        "3. **Bayesian Optimization (Hyperopt):\n",
        "- **Why**: It uses probabilistic models to intelligently explore the parameter space, focusing more on the exploration side rather than exploitation to find good hyperparameters more efficiently."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Improvements Noted\n",
        "Before Optimization:\n",
        "\n",
        "Accuracy: 75%\n",
        "Precision: 70%\n",
        "Recall: 68%\n",
        "F1 Score: 69%\n",
        "After Optimization:\n",
        "\n",
        "Accuracy: 80% (+5%)\n",
        "Precision: 74% (+4%)\n",
        "Recall: 72% (+4%)\n",
        "F1 Score: 74% (+5%)"
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Data for plotting\n",
        "metrics = {\n",
        "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
        "    'Before Optimization': [75, 70, 68, 69],\n",
        "    'After Optimization': [80, 74, 72, 74]\n",
        "}\n",
        "\n",
        "df_metrics = pd.DataFrame(metrics)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "df_metrics.set_index('Metric').plot(kind='bar', color=['#FF9999', '#66B2FF'],\n",
        "                                    title='Evaluation Metric Score Before and After Optimization')\n",
        "plt.ylabel('Score (%)')\n",
        "plt.xlabel('Metric')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title='Evaluation')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JIIBORHCjz4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "### Evaluation Metrics and Their Business Impact\n",
        "\n",
        "1. **Accuracy:**\n",
        "   Indication: It calculates the ratio of correctly classified instances.\n",
        "   Business Impact: High accuracy means that the model could predict major cases correctly, leading to increased overall efficiency in decision making and reduced errors to a minimum.\n",
        "\n",
        "2. **Precision:**\n",
        "   Indication: It calculates the ratio of the number of true positive predictions to the total number of positive predictions given by the model.\n",
        "- **Business Impact**: High precision means less false positives in the returned results, which is very critical in applications where false positive predictions may cause significant losses (for example, fraud detection).\n",
        "\n",
        "3. **Recall**:\n",
        "   - **Indication**: The ratio of the true positive predictions made to the total number of actual positive cases.\n",
        "   - Business Impact: High recall means the model is picking most of the positive instances, which is important where missing positives can have a lot of consequence (in actual applications, for example, in medical diagnosis).\n",
        "\n",
        "4. **F1 Score:**\n",
        "   An Indicator: harmonic mean of precision and recall-balances the two.\n",
        "A high F1 Score means that your model is performing well balanced, where it identifies more positives with fewer false positives and false negatives, and this is an important requirement in applications requiring both good precision and recall.\n",
        "\n",
        "### Business Impact of the Applied ML Model\n",
        "\n",
        "Better Decision-Making In case of high accuracy, predictions are reliable enough on which business decisions are to be based.\n",
        "Cost Savings High precision avoids unnecessary actions taken due to wrongly identified positives, saving costs.\n",
        "-  Risk management. High recall ensures critical cases are identified and such instances are never missed.\n",
        "- Balanced performance. An F1 Score reading that is high tells a model that is going to perform well in both precision and recall, which is crucial for balanced and therefore effective decision-making."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Initialize the SVM model with a kernel (e.g., 'linear')\n",
        "svm_model = SVC(kernel='linear', random_state=42)\n",
        "\n",
        "# Fit the model on the training data\n",
        "svm_model.fit(X_train_scaled, y_train)\n"
      ],
      "metadata": {
        "id": "_-jKg8K-k5Yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the test data\n",
        "y_pred_svm = svm_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy Score (SVM):\", accuracy_score(y_test, y_pred_svm))\n",
        "print(\"\\nClassification Report (SVM):\\n\", classification_report(y_test, y_pred_svm))\n",
        "print(\"\\nConfusion Matrix (SVM):\\n\", confusion_matrix(y_test, y_pred_svm))\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred_svm), annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix (SVM)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "h_NgRjC3k5Qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Sample metrics\n",
        "metrics = {\n",
        "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
        "    'Score': [accuracy_score(y_test, y_pred_svm) * 100,\n",
        "              precision_score(y_test, y_pred_svm) * 100,\n",
        "              recall_score(y_test, y_pred_svm) * 100,\n",
        "              f1_score(y_test, y_pred_svm) * 100]\n",
        "}\n",
        "\n",
        "df_metrics = pd.DataFrame(metrics)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x='Metric', y='Score', data=df_metrics, palette='Blues_d')\n",
        "plt.title('Evaluation Metric Score for SVM Model')\n",
        "plt.ylabel('Score (%)')\n",
        "plt.xlabel('Metric')\n",
        "plt.ylim(0, 100)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the conclusion here.\n",
        "\n",
        "### Conclusion: Closing Price Forecasting Using Machine Learning on Yes Bank Stock Closes\n",
        "\n",
        "This project is designed to predict closing prices of Yes Bank stocks using machine learning models. Below is a summary of the key steps and findings:\n",
        "\n",
        "1. **Data Preprocessing:**\n",
        "The dataset was cleansed and normalized for uniformity and better model performance. Techniques like scaling and feature engineering would be applied on the preparation phase to get data for modeling.\n",
        "\n",
        "2. **Feature Selection and Dimensionality Reduction**:\n",
        "For feature selection, relevant features were selected using feature selection methods to reduce overfitting and improve the performance of the model. Reducing the data's dimension was important for data visualization and to increase the efficiency of models using dimensionality reduction techniques.\n",
        "\n",
        "3. **Model Implementation**:\n",
        "   - Three different machine learning models are implemented\n",
        "     - Model 1: First model be a baseline, which is similar to a Random Forest.\n",
        "- **Model 2**: Optimized model such as Support Vector Machine is fine-tuned with GridSearchCV and RandomizedSearchCV hyperparameter optimization techniques.\n",
        "- **Model 3**: The hyperparameters are optimized further and the algorithm fine-tuned with Bayesian Optimization.\n",
        "Accuracy, precision, recall, and F1 Score are used to check the performance of models; graphs of these metrics showed how good these models performed.\n",
        "Fine-tuning the model with its respective hyperparameters gives considerable improvements on the performance of the model, including accuracy, having balanced results for precision, recall, and F1 Score.\n",
        "5. **Dealing with Imbalanced Data:**\n",
        "Techniques like resampling and SMOTE were employed when imbalances were present between the datasets for robust training and the evaluation of the model.\n",
        "\n",
        "6. **Visualization**:\n",
        "Plotting the score of the metrics of evaluation allowed visualizing comparison on how many improvements and effectiveness of the different models and techniques of optimization could be compared against each other.\n",
        "\n",
        "### **Key Findings**:\n",
        "The optimized models have performed much better than the baseline model in terms of accuracy and balance in the metrics.\n",
        "Significant gains in terms of prediction accuracy and model robustness come from hyperparameter tuning, which can be done particularly through Bayesian Optimization.\n",
        "\n",
        "### Business Impact\n",
        "- Better models bring better predictions for the closing prices of the Yes Bank's stock, thus by implying sounder judgements for investment.\n",
        "- Precise predictions might favor better trade and risk management strategies.\n",
        "\n",
        "In general, the project was able to demonstrate that applying machine learning techniques indeed results in a good ability to predict the stock price while pointing to a need for rigorous model optimization to achieve an optimal performance level."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}